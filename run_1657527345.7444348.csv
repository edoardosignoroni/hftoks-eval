	dataset	lang	tokenizer	vocab_size	train	token
0	en_lt	en	en_char_0.24k	240	3.2710013389587402	62.934088945388794
1	en_lt	lt	lt_char_0.24k	240	3.2438039779663086	59.22366213798523
