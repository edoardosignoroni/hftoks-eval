	dataset	lang	tokenizer	vocab_size	train	token
0	en_mr	en	en_unigram_0.5k	500	1.9633266925811768	1.6917777061462402
1	en_mr	en	en_unigram_0.75k	750	1.8789565563201904	1.6355009078979492
2	en_mr	en	en_unigram_1.5k	1500	1.7485475540161133	1.5395009517669678
3	en_mr	en	en_unigram_3.0k	3000	1.6510679721832275	1.4709043502807617
4	en_mr	en	en_unigram_4.0k	4000	1.6055898666381836	1.7049663066864014
5	en_mr	en	en_unigram_6.0k	6000	1.6999917030334473	1.73233962059021
6	en_mr	en	en_unigram_8.0k	8000	1.4101943969726562	1.8108274936676025
7	en_mr	mr	mr_unigram_0.5k	500	3.3596951961517334	1.905470848083496
8	en_mr	mr	mr_unigram_0.75k	750	2.613562822341919	1.719602108001709
9	en_mr	mr	mr_unigram_1.5k	1500	2.4541242122650146	1.2886435985565186
10	en_mr	mr	mr_unigram_3.0k	3000	2.306685447692871	1.1243102550506592
11	en_mr	mr	mr_unigram_4.0k	4000	2.258772134780884	1.3880419731140137
12	en_mr	mr	mr_unigram_6.0k	6000	2.6929049491882324	2.025014877319336
13	en_mr	mr	mr_unigram_8.0k	8000	2.2523915767669678	1.9206960201263428
