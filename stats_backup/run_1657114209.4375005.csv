	dataset	lang	tokenizer	vocab_size	train	token
0	en_ga	en	en_unigram_0.5k	500	0.8792498111724854	0.5161724090576172
1	en_ga	en	en_unigram_0.75k	750	0.8220865726470947	0.47759437561035156
2	en_ga	en	en_unigram_1.5k	1500	0.7454249858856201	0.5657439231872559
3	en_ga	en	en_unigram_3.0k	3000	0.632347583770752	0.39735960960388184
4	en_ga	en	en_unigram_4.0k	4000	0.5775339603424072	0.4993929862976074
5	en_ga	en	en_unigram_6.0k	6000	0.44005274772644043	0.4362144470214844
6	en_ga	en	en_unigram_8.0k	8000	0.45858287811279297	0.4832277297973633
7	en_ga	ga	ga_unigram_0.5k	500	0.9364960193634033	0.5410687923431396
8	en_ga	ga	ga_unigram_0.75k	750	0.8434443473815918	0.5839176177978516
9	en_ga	ga	ga_unigram_1.5k	1500	0.8349759578704834	0.5363667011260986
10	en_ga	ga	ga_unigram_3.0k	3000	0.7791340351104736	0.5319051742553711
11	en_ga	ga	ga_unigram_4.0k	4000	0.8171765804290771	0.5188853740692139
12	en_ga	ga	ga_unigram_6.0k	6000	0.5955541133880615	0.5444653034210205
13	en_ga	ga	ga_unigram_8.0k	8000	0.5293173789978027	0.5539095401763916
