	dataset	lang	tokenizer	vocab_size	train	token
0	en_mr	en	en_bpe_0.5k	500	0.6830952167510986	2.534324884414673
1	en_mr	en	en_bpe_1.0k	1000	0.6287853717803955	2.531148672103882
2	en_mr	en	en_bpe_2.0k	2000	0.7104344367980957	2.5850274562835693
3	en_mr	en	en_bpe_4.0k	4000	0.8481318950653076	2.6204230785369873
4	en_mr	en	en_bpe_8.0k	8000	1.153085708618164	2.6587488651275635
5	en_mr	en	en_bpe_16.0k	16000	1.8851287364959717	2.8996033668518066
6	en_mr	en	en_bpe_32.0k	32000	3.1072235107421875	2.8596928119659424
7	en_mr	mr	mr_bpe_0.5k	500	0.9255945682525635	2.881436347961426
8	en_mr	mr	mr_bpe_1.0k	1000	1.0191926956176758	3.0490856170654297
9	en_mr	mr	mr_bpe_2.0k	2000	1.3280525207519531	2.7401278018951416
10	en_mr	mr	mr_bpe_4.0k	4000	1.7034833431243896	2.918562173843384
11	en_mr	mr	mr_bpe_8.0k	8000	2.4717822074890137	3.0402300357818604
12	en_mr	mr	mr_bpe_16.0k	16000	4.848332166671753	2.8142433166503906
13	en_mr	mr	mr_bpe_32.0k	32000	10.476167678833008	3.0026471614837646
