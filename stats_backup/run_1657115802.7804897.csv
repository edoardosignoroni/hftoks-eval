	dataset	lang	tokenizer	vocab_size	train	token
0	en_ga	en	en_bpe_0.5k	500	0.43445634841918945	0.9322435855865479
1	en_ga	en	en_bpe_1.0k	1000	0.26401281356811523	0.6606621742248535
2	en_ga	en	en_bpe_2.0k	2000	0.29067063331604004	0.7949750423431396
3	en_ga	en	en_bpe_4.0k	4000	0.36151814460754395	0.6295106410980225
4	en_ga	en	en_bpe_8.0k	8000	0.6028296947479248	0.6901326179504395
5	en_ga	en	en_bpe_16.0k	16000	1.03065824508667	0.7005414962768555
6	en_ga	en	en_bpe_32.0k	32000	1.7384536266326904	0.7359805107116699
7	en_ga	ga	ga_bpe_0.5k	500	0.2539644241333008	0.8324234485626221
8	en_ga	ga	ga_bpe_1.0k	1000	0.3065154552459717	0.7719173431396484
9	en_ga	ga	ga_bpe_2.0k	2000	0.38086414337158203	0.9148497581481934
10	en_ga	ga	ga_bpe_4.0k	4000	0.43952178955078125	0.7096352577209473
11	en_ga	ga	ga_bpe_8.0k	8000	0.706770658493042	0.8065781593322754
12	en_ga	ga	ga_bpe_16.0k	16000	1.1826341152191162	0.7893795967102051
13	en_ga	ga	ga_bpe_32.0k	32000	1.903784990310669	0.8090353012084961
