{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "VVV BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data. Still\n",
    "    BPE uses final vocabulary size in sentence piece, even when another model cannot use that size\n",
    "            \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > I need:\n",
    "        > vocab_size\n",
    "        > length \n",
    "        > freq@95%\n",
    "    > check the correctness of freq@95 and avg_len stats\n",
    "        > get percentiles of used tokens\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "    > grid plots, change df to include dataset, model, value\n",
    "    > must plot all things together\n",
    "\n",
    "!!! better self contained functions\n",
    "    > selective run to be passed in init\n",
    "    > separate make_freqs from tokenization\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type, data_path):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn ./{self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "   \n",
    "    def tokenize(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenize train for lang\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "        \n",
    "            train_path = f'./{self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            out = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'\n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            if os.path.isfile(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "            \n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as text:\n",
    "                \n",
    "                start = time.time()               \n",
    "                for line in text:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()   \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return(end-start)\n",
    "\n",
    "    def make_freqs(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                \n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = int(line[1].strip('\\n'))\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "            \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "            \n",
    "            start=time.time()\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "            freqs = {}\n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'r') as f:\n",
    "                for line in f:\n",
    "                    for piece in line:\n",
    "                        freqs.setdefault(piece, 0)\n",
    "                        freqs[piece] += 1\n",
    "                        \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "\n",
    "    def run(self, langs=None, vocab_sizes=None, train=True, tokenize=True, freqs=True):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "        if not langs:\n",
    "            langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        for lang in langs:\n",
    "           \n",
    "            if not vocab_sizes:\n",
    "            \n",
    "                if self.model_type=='char':\n",
    "                    vocab_sizes = [self.count_chars(lang)]\n",
    "                elif self.model_type=='bpe': #merge operations\n",
    "                    vocab_sizes = [500,\n",
    "                                   1000,\n",
    "                                   2000,\n",
    "                                   4000,\n",
    "                                   8000, \n",
    "                                   16000,\n",
    "                                   32000, \n",
    "                                   #48000, too big for en-ga\n",
    "                                   #64000 too big for en-mr\n",
    "                                   ]\n",
    "                elif self.model_type=='unigram': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "                elif self.model_type=='hft': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                if train:\n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                if tokenize:\n",
    "                    token_time = self.tokenize(lang, tokenizer_name)                   \n",
    "                if freqs: \n",
    "                    self.make_freqs(lang, tokenizer_name)\n",
    "                \n",
    "                row = {'dataset':self.pair, 'lang':lang, 'tokenizer':tokenizer_name, 'vocab_size':size, 'train':train_time, 'token':token_time}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "                \n",
    "        df.to_csv(f'./run_{time.time()}.csv', sep='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lt_en bpe\n",
      "Training tokenizer for en with vocab_size of 500\n",
      "Training time: 8.015945672988892\n",
      "en text tokenized in 47.78504824638367 with en_bpe_0.5k\n",
      "Made freqs for en_bpe_0.5k in 18.73268747329712\n",
      "Training tokenizer for en with vocab_size of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.47726321220398\n",
      "en text tokenized in 74.31485342979431 with en_bpe_1.0k\n",
      "Made freqs for en_bpe_1.0k in 15.750118017196655\n",
      "Training tokenizer for en with vocab_size of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.013867855072021\n",
      "en text tokenized in 72.7385926246643 with en_bpe_2.0k\n",
      "Made freqs for en_bpe_2.0k in 14.896342515945435\n",
      "Training tokenizer for en with vocab_size of 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.733312606811523\n",
      "en text tokenized in 75.86577677726746 with en_bpe_4.0k\n",
      "Made freqs for en_bpe_4.0k in 13.72604775428772\n",
      "Training tokenizer for en with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 11.868770122528076\n",
      "en text tokenized in 82.51929187774658 with en_bpe_8.0k\n",
      "Made freqs for en_bpe_8.0k in 13.161970853805542\n",
      "Training tokenizer for en with vocab_size of 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 14.215688228607178\n",
      "en text tokenized in 91.46743273735046 with en_bpe_16.0k\n",
      "Made freqs for en_bpe_16.0k in 12.913257837295532\n",
      "Training tokenizer for en with vocab_size of 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 15.332538843154907\n",
      "en text tokenized in 75.91682291030884 with en_bpe_32.0k\n",
      "Made freqs for en_bpe_32.0k in 12.726557731628418\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/1544986493.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            #'en_hi',\n",
    "            'lt_en'\n",
    "           ]\n",
    "model_types = [\n",
    "              #'char',\n",
    "              #'unigram',\n",
    "              'bpe',\n",
    "              #'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type, data_path='./data_big')\n",
    "        model.run(langs=['en'])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset, just_tgt=False,):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = f'./data_big/{dataset}'\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        self.just_tgt = just_tgt\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \n",
    "        langs = self.pair\n",
    "        if self.just_tgt:\n",
    "            langs = [langs[1]]\n",
    "       \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in langs:\n",
    "            tokenizers = {} # tokenizer : ( freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "               # if \"hft\" not in path: #to remove after hftoks implementation\n",
    "\n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    \n",
    "                    if 'hft' in path:\n",
    "                        train = f'{self.dataset_dir}/train/tokenized/hft_pretokenized.{lang}'\n",
    "                    else:\n",
    "                        train = f'{self.dataset_dir}/train.{lang}'\n",
    "                    \n",
    "                    tokenized = f'{self.dataset_dir}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "        \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    \n",
    "                freqs_path = paths[lang][tokenizer][0]\n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "   \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "\n",
    "                freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                lines = tokenized_text.readlines()\n",
    "\n",
    "                if 'hft' in tokenizer:\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(' ')\n",
    "                        new_line = [i for i in line if i not in [\"êãá\",\"‚ñÅ\",\"êä£\",\"êäº\"]]\n",
    "                        avg_len += len(new_line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "                    \n",
    "                else:    \n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "                if \"unigram\" in tokenizer:\n",
    "                    tokenizer_type = \"unigram\"\n",
    "                elif \"bpe\" in tokenizer:\n",
    "                    tokenizer_type = \"bpe\"\n",
    "                elif \"char\" in tokenizer:\n",
    "                    tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                elif \"hft\" in tokenizer:\n",
    "                    tokenizer_type = \"hft\"\n",
    "                    \n",
    "\n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                       \"lang\" : lang,                  \n",
    "                       \"tokenizer\" : tokenizer_type,\n",
    "                       \"vocab_size\" : vocab_size,\n",
    "                       \"freq@95%\" : freq_at_95,\n",
    "                       \"avg_len\" : avg_len}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./{self.dataset}.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n",
    "    \n",
    "    def plot(self, value):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \n",
    "        plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\n",
    "        do not matter\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.collect_stats()\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.lineplot(data=df,\n",
    "                    x=\"vocab_size\", y=value, hue=\"tokenizer\", style=\"tokenizer\",\n",
    "                    ci=None, markers=True, dashes=False, palette=\"tab10\", linewidth=2.5, sort=True)\n",
    "        \n",
    "        ax.set(title={self.dataset},\n",
    "                    xlabel=\"Vocabulary size\",\n",
    "                    ylabel=value,\n",
    "                    )\n",
    "        if value == 'freq@95%':\n",
    "            ax.invert_yaxis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga', just_tgt=False)\n",
    "p2 = Plotter('en_mr', just_tgt=False)\n",
    "p3 = Plotter('en_hi', just_tgt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5f73cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_20263/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>lang</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>freq@95%</th>\n",
       "      <th>avg_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.669924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.68101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.327669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.517914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.802882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.99497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.99497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.541738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.527349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.650875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>char</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1</td>\n",
       "      <td>73.86122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>char</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.373969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.744323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.727697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.331665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>34.485302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.049859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.225286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.29596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.915175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23.374626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.4945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>29.638249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.552261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.850645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.654986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>18</td>\n",
       "      <td>28.111614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.363853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.753862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.229319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>550</td>\n",
       "      <td>26.509804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.912189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>19.676049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.809592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hft</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>502</td>\n",
       "      <td>25.666028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>18.598691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>unigram</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.993392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.039494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.311612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.357872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.72638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.628532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.56387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en_hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bpe</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.700231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset lang tokenizer vocab_size freq@95%    avg_len\n",
       "2    en_hi   en       bpe      500.0        1  33.669924\n",
       "6    en_hi   en   unigram      500.0        1   33.68101\n",
       "34   en_hi   hi       hft      500.0        1  57.327669\n",
       "33   en_hi   hi   unigram      500.0        1  36.517914\n",
       "25   en_hi   hi       bpe      500.0        1  36.802882\n",
       "16   en_hi   en       hft      500.0        1   50.99497\n",
       "21   en_hi   en       hft      750.0        1   50.99497\n",
       "38   en_hi   hi   unigram      750.0        1  32.541738\n",
       "24   en_hi   hi       hft      750.0        1  44.527349\n",
       "15   en_hi   en   unigram      750.0        1  30.650875\n",
       "40   en_hi   hi      char      785.0        1   73.86122\n",
       "4    en_hi   en      char      785.0        1  76.373969\n",
       "3    en_hi   en       bpe     1000.0        1  28.744323\n",
       "39   en_hi   hi       bpe     1000.0        1  30.727697\n",
       "35   en_hi   hi   unigram     1500.0        1  27.331665\n",
       "30   en_hi   hi       hft     1500.0        2  34.485302\n",
       "0    en_hi   en   unigram     1500.0        1  26.049859\n",
       "10   en_hi   en       hft     1500.0        1  29.225286\n",
       "41   en_hi   hi       bpe     2000.0        1   26.29596\n",
       "13   en_hi   en       bpe     2000.0        1  24.915175\n",
       "11   en_hi   en       hft     3000.0        2  23.374626\n",
       "32   en_hi   hi   unigram     3000.0        1    23.4945\n",
       "36   en_hi   hi       hft     3000.0        6  29.638249\n",
       "7    en_hi   en   unigram     3000.0        1  22.552261\n",
       "31   en_hi   hi       bpe     4000.0        1  22.850645\n",
       "8    en_hi   en       hft     4000.0        3  21.654986\n",
       "27   en_hi   hi       hft     4000.0       18  28.111614\n",
       "9    en_hi   en   unigram     4000.0        1  21.363853\n",
       "1    en_hi   en       bpe     4000.0        1  21.753862\n",
       "42   en_hi   hi   unigram     4000.0        1  22.229319\n",
       "28   en_hi   hi       hft     6000.0      550  26.509804\n",
       "20   en_hi   en   unigram     6000.0        1  19.912189\n",
       "18   en_hi   en       hft     6000.0        6  19.676049\n",
       "26   en_hi   hi   unigram     6000.0        1  20.809592\n",
       "37   en_hi   hi       hft     8000.0      502  25.666028\n",
       "17   en_hi   en       hft     8000.0       19  18.598691\n",
       "23   en_hi   hi   unigram     8000.0        1  19.993392\n",
       "19   en_hi   en   unigram     8000.0        1  19.039494\n",
       "14   en_hi   en       bpe     8000.0        1  19.311612\n",
       "43   en_hi   hi       bpe     8000.0        1  20.357872\n",
       "29   en_hi   hi       bpe    16000.0        1   18.72638\n",
       "5    en_hi   en       bpe    16000.0        1  17.628532\n",
       "12   en_hi   en       bpe    32000.0        1   16.56387\n",
       "22   en_hi   hi       bpe    32000.0        1  17.700231"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
