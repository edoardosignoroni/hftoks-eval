{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "VVV BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data\n",
    "            \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > I need:\n",
    "        > vocab_size\n",
    "        > length \n",
    "        > freq@95%\n",
    "    > check the correctness of freq@95 and avg_len stats\n",
    "        > check freq and vocab on hft, ask pavel\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "    > grid plots, change df to include dataset, model, value\n",
    "    > must plot all things together\n",
    "\n",
    "!!! better self contained functions\n",
    "    > selective run to be passed in init\n",
    "    > separate make_freqs from tokenization\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type, data_path):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0) #problems with pretokenize in loop\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "        print(f'Training time: {end-start}')\n",
    "\n",
    "       \n",
    "        return (end-start)\n",
    "       \n",
    "    def make_freqs(self, lang, tokenizer, save_tokenized=False):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            \n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                \n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = line[1].strip('\\n')\n",
    "            \n",
    "            train_path = f'./{self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            out = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'\n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            \n",
    "            return (end-start)\n",
    "            \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "            if save_tokenized == True:\n",
    "                if os.path.isfile(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "\n",
    "            freqs = {}\n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as f:\n",
    "                start = time.time()\n",
    "                for line in f:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    for piece in toks:\n",
    "                        freqs.setdefault(piece, 0)\n",
    "                        freqs[piece] += 1\n",
    "\n",
    "                    if save_tokenized == True:\n",
    "                        with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()\n",
    "\n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "\n",
    "    def run(self, langs=None, vocab_sizes=None, train=True, freqs=True):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "        if not langs:\n",
    "            langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        for lang in langs:\n",
    "           \n",
    "            if not vocab_sizes:\n",
    "            \n",
    "                if self.model_type=='char':\n",
    "                    vocab_sizes = [self.count_chars(lang)]\n",
    "                elif self.model_type=='bpe': #merge operations\n",
    "                    vocab_sizes = [500,\n",
    "                                   1000,\n",
    "                                   2000,\n",
    "                                   4000,\n",
    "                                   8000, \n",
    "                                   16000,\n",
    "                                   32000, \n",
    "                                   #48000, too big for en-ga\n",
    "                                   #64000 too big for en-mr\n",
    "                                   ]\n",
    "                elif self.model_type=='unigram': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "                elif self.model_type=='hft': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                if train:\n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                   \n",
    "                if freqs: \n",
    "                    token_time = self.make_freqs(lang, tokenizer_name, save_tokenized=True)\n",
    "                \n",
    "                row = {'dataset':self.pair, 'lang':lang, 'tokenizer':tokenizer_name, 'vocab_size':size, 'train':train_time, 'token':token_time}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "                \n",
    "        df.to_csv(f'./run_{time.time()}.csv', sep='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_hi unigram\n",
      "Training tokenizer for en with vocab_size of 500\n",
      "Training time: 83.24014282226562\n",
      "en text tokenized in 128.69902563095093 with en_unigram_0.5k\n",
      "Training tokenizer for en with vocab_size of 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 88.68234300613403\n",
      "en text tokenized in 135.98023533821106 with en_unigram_0.75k\n",
      "Training tokenizer for en with vocab_size of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 82.14736652374268\n",
      "en text tokenized in 126.69090938568115 with en_unigram_1.5k\n",
      "Training tokenizer for en with vocab_size of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 82.69791746139526\n",
      "en text tokenized in 135.8418483734131 with en_unigram_3.0k\n",
      "Training tokenizer for en with vocab_size of 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 89.6457052230835\n",
      "en text tokenized in 128.86155438423157 with en_unigram_4.0k\n",
      "Training tokenizer for en with vocab_size of 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 88.22702145576477\n",
      "en text tokenized in 127.8052589893341 with en_unigram_6.0k\n",
      "Training tokenizer for en with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 90.6580274105072\n",
      "en text tokenized in 135.94504356384277 with en_unigram_8.0k\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20572/2247101937.py:249: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            'en_hi',\n",
    "            #'lt_en'\n",
    "           ]\n",
    "model_types = [\n",
    "              #'char',\n",
    "              'unigram',\n",
    "              #'bpe',\n",
    "              #'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type, data_path='./data_big')\n",
    "        model.run(langs=['en'])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset, just_tgt=False):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = f'./{self.data_path}/{dataset}'\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        self.just_tgt = just_tgt\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \n",
    "        langs = self.pair\n",
    "        if self.just_tgt:\n",
    "            langs = [langs[1]]\n",
    "       \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in langs:\n",
    "            tokenizers = {} # tokenizer : ( freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "               # if \"hft\" not in path: #to remove after hftoks implementation\n",
    "\n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    \n",
    "                    if 'hft' in path:\n",
    "                        train = f'{self.dataset_dir}/train/tokenized/hft_pretokenized.{lang}'\n",
    "                    else:\n",
    "                        train = f'{self.dataset_dir}/train.{lang}'\n",
    "                    \n",
    "                    tokenized = f'{self.dataset_dir}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "        \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    \n",
    "                freqs_path = paths[lang][tokenizer][0]\n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "   \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "\n",
    "                freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                lines = tokenized_text.readlines()\n",
    "\n",
    "                if 'hft' in tokenizer:\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(' ')\n",
    "                        new_line = [i for i in line if i not in [\"êãá\",\"‚ñÅ\",\"êä£\",\"êäº\"]]\n",
    "                        avg_len += len(new_line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "                    \n",
    "                else:    \n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "                if \"unigram\" in tokenizer:\n",
    "                    tokenizer_type = \"unigram\"\n",
    "                elif \"bpe\" in tokenizer:\n",
    "                    tokenizer_type = \"bpe\"\n",
    "                elif \"char\" in tokenizer:\n",
    "                    tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                elif \"hft\" in tokenizer:\n",
    "                    tokenizer_type = \"hft\"\n",
    "                    \n",
    "\n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                       \"lang\" : lang,                  \n",
    "                       \"tokenizer\" : tokenizer_type,\n",
    "                       \"vocab_size\" : vocab_size,\n",
    "                       \"freq@95%\" : freq_at_95,\n",
    "                       \"avg_len\" : avg_len}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open('./df_test2.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n",
    "    \n",
    "    def plot(self, value):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \n",
    "        plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\n",
    "        do not matter\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.collect_stats()\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.lineplot(data=df,\n",
    "                    x=\"vocab_size\", y=value, hue=\"tokenizer\", style=\"tokenizer\",\n",
    "                    ci=None, markers=True, dashes=False, palette=\"tab10\", linewidth=2.5, sort=True)\n",
    "        \n",
    "        ax.set(title={self.dataset},\n",
    "                    xlabel=\"Vocabulary size\",\n",
    "                    ylabel=value,\n",
    "                    )\n",
    "        if value == 'freq@95%':\n",
    "            ax.invert_yaxis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga', just_tgt=False)\n",
    "p2 = Plotter('en_mr', just_tgt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot('freq@95%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.plot('freq@95%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99689506",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot('avg_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d25509",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.plot('avg_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
