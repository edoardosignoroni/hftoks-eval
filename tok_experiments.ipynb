{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type, data_path):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \n",
    "        if hft, must pretokenize beforehand\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn {self.data_path}/{self.pair}/train/tokenized/train_hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            manychars = ['ja']\n",
    "            \n",
    "            charcover = 1.0\n",
    "            \n",
    "            if lang in manychars:\n",
    "                charcover = 0.98\n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "            \n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                character_coverage=charcover,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "   \n",
    "    def tokenize(self, lang, tokenizer, prefix):\n",
    "        \"\"\"\n",
    "        Tokenize train for lang with tokenizer (prefix is used to tokenize \"train\" or \"dev\")\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "        \n",
    "            train_path = f'{self.data_path}/{self.pair}/train/tokenized/{prefix}_hft_pretokenized.{lang}'\n",
    "            \n",
    "            dev_path = f'{self.data_path}/{self.pair}/train/tokenized/{prefix}_hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            out = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/{prefix}_toks_{tokenizer}.{lang}'\n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            if os.path.isfile(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/{prefix}_toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/{prefix}_toks_{tokenizer}.{lang}')\n",
    "            \n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/{prefix}.{lang}', 'r') as text:\n",
    "                \n",
    "                start = time.time()               \n",
    "                for line in text:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/{prefix}_toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()   \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return(end-start)\n",
    "\n",
    "    def make_freqs(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = int(line[1].strip('\\n'))\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "            \n",
    "        else:\n",
    "            #correct for prefix\n",
    "            start=time.time()\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "            tokenized_path = f'{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/train_toks_{tokenizer}.{lang}'\n",
    "        \n",
    "        \n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            toks = open(tokenized_path, 'r').readlines()\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "            \n",
    "            freqs = {}\n",
    "            \n",
    "            for line in toks:\n",
    "                line = ast.literal_eval(line)\n",
    "                for tok in line:\n",
    "                    #print (tok)\n",
    "                    if tok in vocabs:\n",
    "                        if tok in freqs.keys():\n",
    "                            freqs[tok] = freqs[tok]+1\n",
    "                        else:\n",
    "                            freqs[tok] = 1\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "\n",
    "    def run(self, langs=None, vocab_sizes=None, train=True, tokenize=True, dev=False, freqs=True, save_run=True):\n",
    "        \"\"\"\n",
    "        Runs the other functions, returns train time and tokenization time\n",
    "        \"\"\"\n",
    "        \n",
    "        if not langs:\n",
    "            langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "        \n",
    "        for lang in langs:\n",
    "           \n",
    "            if not vocab_sizes:\n",
    "                # char model is not useful\n",
    "                if self.model_type=='char':\n",
    "                    vocab_sizes = [self.count_chars(lang)]\n",
    "                elif self.model_type=='bpe': #merge operations\n",
    "                    vocab_sizes = [\n",
    "                                   #500,\n",
    "                                   1000,\n",
    "                                   2000,\n",
    "                                   4000,\n",
    "                                   8000, \n",
    "                                   16000,\n",
    "                                   32000, \n",
    "                                   #48000, too big for en-ga\n",
    "                                   #64000 too big for en-mr\n",
    "                                   ]\n",
    "                elif self.model_type=='unigram': #final vocabulary size\n",
    "                    vocab_sizes = [\n",
    "                                   500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "                elif self.model_type=='hft': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "            \n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                train_time = 0\n",
    "                token_time = 0\n",
    "                if train:\n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                if tokenize:\n",
    "                    if dev == True:\n",
    "                        token_time = self.tokenize(lang, tokenizer_name, 'dev')\n",
    "                    else:\n",
    "                        token_time = self.tokenize(lang, tokenizer_name, 'train')\n",
    "                if freqs:\n",
    "                        self.make_freqs(lang, tokenizer_name) \n",
    "                if save_run:\n",
    "                    row = {'dataset':self.pair, 'lang':lang, 'tokenizer':tokenizer_name, 'vocab_size':size, 'train':train_time, 'token':token_time}\n",
    "                    df = df.append(row, ignore_index=True)\n",
    "        if save_run:        \n",
    "            df.to_csv(f'./stats/{self.pair}_time.csv', sep='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d439bfb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs_sv unigram\n",
      "Training tokenizer for cs with vocab_size of 500\n",
      "Training time: 3.8944690227508545\n",
      "cs text tokenized in 1.9679884910583496 with cs_unigram_0.5k\n",
      "Made freqs for cs_unigram_0.5k in 2.7274913787841797\n",
      "Training tokenizer for cs with vocab_size of 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.505624294281006\n",
      "cs text tokenized in 1.997018814086914 with cs_unigram_0.75k\n",
      "Made freqs for cs_unigram_0.75k in 2.8336851596832275\n",
      "Training tokenizer for cs with vocab_size of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.46278977394104\n",
      "cs text tokenized in 1.853463888168335 with cs_unigram_1.5k\n",
      "Made freqs for cs_unigram_1.5k in 3.190277576446533\n",
      "Training tokenizer for cs with vocab_size of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.196333646774292\n",
      "cs text tokenized in 1.7011232376098633 with cs_unigram_3.0k\n",
      "Made freqs for cs_unigram_3.0k in 4.088312149047852\n",
      "Training tokenizer for cs with vocab_size of 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.115480661392212\n",
      "cs text tokenized in 1.6884493827819824 with cs_unigram_4.0k\n",
      "Made freqs for cs_unigram_4.0k in 4.579796075820923\n",
      "Training tokenizer for cs with vocab_size of 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.9848227500915527\n",
      "cs text tokenized in 1.6737074851989746 with cs_unigram_6.0k\n",
      "Made freqs for cs_unigram_6.0k in 5.559765577316284\n",
      "Training tokenizer for cs with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.8677000999450684\n",
      "cs text tokenized in 1.6866333484649658 with cs_unigram_8.0k\n",
      "Made freqs for cs_unigram_8.0k in 6.444267749786377\n",
      "Training tokenizer for sv with vocab_size of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.6655242443084717\n",
      "sv text tokenized in 1.9195024967193604 with sv_unigram_0.5k\n",
      "Made freqs for sv_unigram_0.5k in 2.821676254272461\n",
      "Training tokenizer for sv with vocab_size of 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.6190483570098877\n",
      "sv text tokenized in 2.026533365249634 with sv_unigram_0.75k\n",
      "Made freqs for sv_unigram_0.75k in 2.8850386142730713\n",
      "Training tokenizer for sv with vocab_size of 1500\n",
      "Training time: 2.882889986038208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv text tokenized in 1.8612146377563477 with sv_unigram_1.5k\n",
      "Made freqs for sv_unigram_1.5k in 3.3034181594848633\n",
      "Training tokenizer for sv with vocab_size of 3000\n",
      "Training time: 2.457979679107666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv text tokenized in 1.7699251174926758 with sv_unigram_3.0k\n",
      "Made freqs for sv_unigram_3.0k in 3.8328843116760254\n",
      "Training tokenizer for sv with vocab_size of 4000\n",
      "Training time: 2.3484950065612793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv text tokenized in 1.7398505210876465 with sv_unigram_4.0k\n",
      "Made freqs for sv_unigram_4.0k in 4.246295213699341\n",
      "Training tokenizer for sv with vocab_size of 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.20053768157959\n",
      "sv text tokenized in 1.7809720039367676 with sv_unigram_6.0k\n",
      "Made freqs for sv_unigram_6.0k in 4.867952585220337\n",
      "Training tokenizer for sv with vocab_size of 8000\n",
      "Training time: 2.0801753997802734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv text tokenized in 1.9306962490081787 with sv_unigram_8.0k\n",
      "Made freqs for sv_unigram_8.0k in 5.845574855804443\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33781/174820049.py:296: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cannot run all together bacause jupyter saves outputs checkpoints, stalling the console. Run one dataset at a time,\n",
    "one model at a time, or pass one lang.\n",
    "\"\"\"\n",
    "\n",
    "datasets = [\n",
    "            #'en_mr',\n",
    "            #'en_ga',\n",
    "            #\"en_hi\",\n",
    "            #\"en_lt\",\n",
    "            #\"ja_my\",\n",
    "            'cs_sv',\n",
    "            #'gbi_chr',\n",
    "            #'syr_zu'\n",
    "            ]\n",
    "model_types = [\n",
    "              #'char',\n",
    "              'unigram',\n",
    "              #'bpe',\n",
    "              #'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type, data_path='./data')\n",
    "        model.run(langs=[lang for lang in dataset.split('_')],\n",
    "                  #train=False,\n",
    "                  #tokenize=True,\n",
    "                  #dev=True,\n",
    "                  #freqs=False,\n",
    "                  #save_run=False\n",
    "                 )\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    \"\"\"\n",
    "    Called Plotter for previous versions, DOES NOT PLOT (use other notebook). Generates .csv with statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, dataset_dir, just_tgt=False):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        self.just_tgt = just_tgt\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary of paths of relevant files\n",
    "        \"\"\"\n",
    "        \n",
    "        langs = self.pair\n",
    "        if self.just_tgt:\n",
    "            langs = [langs[1]]\n",
    "       \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in langs:\n",
    "            tokenizers = {} # tokenizer : (freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "                \n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    \n",
    "                    if 'hft' in path:\n",
    "                        train = f'{self.dataset_dir}/{self.dataset}/train/tokenized/train_hft_pretokenized.{lang}'\n",
    "                    else:\n",
    "                        train = f'{self.dataset_dir}/{self.dataset}/train.{lang}'\n",
    "                    \n",
    "                    tokenized = f'{self.dataset_dir}/{self.dataset}/train/tokenized/train_toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "    \n",
    "    \n",
    "    def collect_percs(self):\n",
    "        \"\"\"\n",
    "        This should return the amount of tokens with equal or less freq than 0, 10, 100. Not fully implemented yet, I think\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', '0', '10', '100'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                vocab = open(f'{self.tokenizers_dir}/{lang}/{tokenizer}/{tokenizer}.vocab', 'r').readlines()\n",
    "                freqs_path = f'{self.tokenizers_dir}/{lang}/{tokenizer}/{tokenizer}.freq'\n",
    "                freqs = ast.literal_eval(open(freqs_path, 'r').read())\n",
    "                \"\"\"print(tokenizer)\n",
    "                print('freqs', len(freqs.values()))\n",
    "                print('vocab', len(vocab))\"\"\"\n",
    "                \n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "                \n",
    "                zeros = len(vocab)-len(freqs.values())\n",
    "                tens = 0\n",
    "                hundr = 0\n",
    "                \n",
    "                for value in freqs.values():\n",
    "                    if int(value) <= 10:\n",
    "                        tens += 1\n",
    "                    if int(value) <= 100:\n",
    "                        hundr += 1\n",
    "                \n",
    "                zeros = (zeros/vocab_size)*100\n",
    "                tens = (tens/vocab_size)*100\n",
    "                hundr = (hundr/vocab_size)*100\n",
    "                        \n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                   \"lang\" : lang,                  \n",
    "                   \"tokenizer\" : tokenizer,\n",
    "                   \"vocab_size\" : vocab_size,\n",
    "                   \"0\" : zeros,\n",
    "                   \"10\" : tens,\n",
    "                    \"100\" : hundr}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        print(df.head())\n",
    "        \n",
    "        with open(f'./{self.dataset}_percs.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "    \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        Collects the stats and generates a .csv\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    \n",
    "                freqs_path = paths[lang][tokenizer][0]\n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "   \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "                \n",
    "                freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                lines = tokenized_text.readlines()\n",
    "\n",
    "                if 'hft' in tokenizer:\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(' ')\n",
    "                        avg_len += len(line)\n",
    "                        \n",
    "                    avg_len = avg_len/len(lines)\n",
    "                \n",
    "                else:\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "                if \"unigram\" in tokenizer:\n",
    "                    tokenizer_type = \"unigram\"\n",
    "                elif \"bpe\" in tokenizer:\n",
    "                    tokenizer_type = \"bpe\"\n",
    "                elif \"char\" in tokenizer:\n",
    "                    tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                elif \"hft\" in tokenizer:\n",
    "                    tokenizer_type = \"hft\"\n",
    "                    \n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                       \"lang\" : lang,                  \n",
    "                       \"tokenizer\" : tokenizer_type,\n",
    "                       \"vocab_size\" : vocab_size,\n",
    "                       \"freq@95%\" : freq_at_95,\n",
    "                       \"avg_len\" : avg_len}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"tokenizer\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./stats/{self.dataset}.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n",
    "    \n",
    "    def run(self, percs=True, stats=True):\n",
    "        \"\"\"\n",
    "        runs other functions\n",
    "        \"\"\"\n",
    "        \n",
    "        if percs:\n",
    "            self.collect_percs()\n",
    "        if stats:\n",
    "            self.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "            #'en_mr',\n",
    "            #'en_ga',\n",
    "            #\"en_hi\",\n",
    "            #\"en_lt\",\n",
    "            \"ja_my\"\n",
    "            ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    Plotter(dataset, './data').run(percs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    TBA, future work\n",
    "    \n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def big_to_small(pair, size):\n",
    "    \"\"\"\n",
    "    generates a sample of a bigger file according to language pair and size\n",
    "    \"\"\"\n",
    "    random.seed(20220713)\n",
    "    \n",
    "    pair = pair.split('_')\n",
    "\n",
    "    big_path1 = f'./data_big/{pair[0]}_{pair[1]}/train.{pair[0]}'\n",
    "    big_file1 = open(big_path1, 'r').readlines()\n",
    "    small_path1 = f'./data/{pair[0]}_{pair[1]}/train.{pair[0]}'\n",
    "    \n",
    "    sample = random.sample(range(len(big_file1)), size)\n",
    "    \n",
    "    big_path2 = f'./data_big/{pair[0]}_{pair[1]}/train.{pair[1]}'\n",
    "    big_file2 = open(big_path2, 'r').readlines()\n",
    "    small_path2 = f'./data/{pair[0]}_{pair[1]}/train.{pair[1]}'\n",
    "    \n",
    "    #os.mkdir(f'./data/{pair[0]}_{pair[1]}')\n",
    "\n",
    "    with open(small_path1, 'w+') as small_file1:\n",
    "        for i in sample:\n",
    "            print(big_file1[i].strip('\\n'), file=small_file1)\n",
    "    \n",
    "    with open(small_path2, 'w+') as small_file2:\n",
    "        for i in sample:\n",
    "            print(big_file2[i].strip('\\n'), file=small_file2) \n",
    "\n",
    "def xml_to_raw(in_file, out_file):\n",
    "    \"\"\"\n",
    "    generates a raw txt from a xml\n",
    "    \"\"\"\n",
    "    \n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    with open(out_file, 'w+') as out:\n",
    "        for child in root.iter('s'):\n",
    "            \n",
    "            print(child.text, file=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b10ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_to_raw('./data/syr_zu/Zulu-NT.xml', './data/syr_zu/train.zu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b36c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
