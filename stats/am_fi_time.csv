	dataset	lang	tokenizer	vocab_size	train	token
0	am_fi	am	am_hft_0.5k	500	6.615774154663086	4.950775623321533
1	am_fi	am	am_hft_0.75k	750	11.730978012084961	5.000396490097046
2	am_fi	am	am_hft_1.5k	1500	27.82786989212036	5.693268060684204
3	am_fi	am	am_hft_3.0k	3000	64.85455417633057	5.767495393753052
4	am_fi	am	am_hft_4.0k	4000	89.11194586753845	5.557163715362549
5	am_fi	am	am_hft_6.0k	6000	142.65651845932007	5.968047618865967
6	am_fi	am	am_hft_8.0k	8000	198.19385743141174	6.165158987045288
7	am_fi	fi	fi_hft_0.5k	500	12.363992691040039	9.790909767150879
8	am_fi	fi	fi_hft_0.75k	750	18.562262058258057	10.454125881195068
9	am_fi	fi	fi_hft_1.5k	1500	37.54849076271057	11.013959169387817
10	am_fi	fi	fi_hft_3.0k	3000	83.86984419822693	11.374129295349121
11	am_fi	fi	fi_hft_4.0k	4000	114.11491560935974	11.876617908477783
12	am_fi	fi	fi_hft_6.0k	6000	179.42553210258484	12.904471158981323
13	am_fi	fi	fi_hft_8.0k	8000	248.7219579219818	12.588098287582397
