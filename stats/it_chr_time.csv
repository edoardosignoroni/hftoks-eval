	dataset	lang	tokenizer	vocab_size	train	token
0	it_chr	it	it_hft_0.5k	500	7.005022048950195	10.634994268417358
1	it_chr	it	it_hft_0.75k	750	11.498806476593018	10.853565454483032
2	it_chr	it	it_hft_1.5k	1500	24.93313431739807	11.803411483764648
3	it_chr	it	it_hft_3.0k	3000	53.89702749252319	11.844175100326538
4	it_chr	it	it_hft_4.0k	4000	76.79645776748657	12.510571241378784
5	it_chr	it	it_hft_6.0k	6000	120.50925707817078	12.709486722946167
6	it_chr	it	it_hft_8.0k	8000	158.72302436828613	11.72645378112793
7	it_chr	chr	chr_hft_0.5k	500	4.144804239273071	1.6313226222991943
8	it_chr	chr	chr_hft_0.75k	750	7.245588541030884	1.6514239311218262
9	it_chr	chr	chr_hft_1.5k	1500	15.471378326416016	1.8465206623077393
10	it_chr	chr	chr_hft_3.0k	3000	33.70989966392517	1.8501622676849365
11	it_chr	chr	chr_hft_4.0k	4000	45.74470329284668	1.983548879623413
12	it_chr	chr	chr_hft_6.0k	6000	71.42894506454468	1.9824843406677246
13	it_chr	chr	chr_hft_8.0k	8000	102.34164214134216	2.0744879245758057
