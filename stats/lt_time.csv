	lang	tokenizer	vocab_size	train	token
0	lt	lt_unigram_hft_pretok_0.5k	500	2.126861810684204	1.5124483108520508
1	lt	lt_unigram_hft_pretok_0.75k	750	1.9431521892547607	1.297607183456421
2	lt	lt_unigram_hft_pretok_1.5k	1500	1.9588127136230469	1.2584433555603027
3	lt	lt_unigram_hft_pretok_3.0k	3000	1.8981077671051025	1.2870478630065918
4	lt	lt_unigram_hft_pretok_4.0k	4000	1.7821757793426514	1.2234153747558594
5	lt	lt_unigram_hft_pretok_6.0k	6000	1.5774166584014893	1.1541225910186768
6	lt	lt_unigram_hft_pretok_8.0k	8000	1.3784849643707275	1.181380271911621
