	dataset	lang	tokenizer	vocab_size	train	token
0	cs_sv	cs	cs_unigram_0.5k	500	3.8944690227508545	1.9679884910583496
1	cs_sv	cs	cs_unigram_0.75k	750	3.505624294281006	1.997018814086914
2	cs_sv	cs	cs_unigram_1.5k	1500	3.46278977394104	1.853463888168335
3	cs_sv	cs	cs_unigram_3.0k	3000	3.196333646774292	1.7011232376098633
4	cs_sv	cs	cs_unigram_4.0k	4000	3.115480661392212	1.6884493827819824
5	cs_sv	cs	cs_unigram_6.0k	6000	2.9848227500915527	1.6737074851989746
6	cs_sv	cs	cs_unigram_8.0k	8000	2.8677000999450684	1.6866333484649658
7	cs_sv	sv	sv_unigram_0.5k	500	2.6655242443084717	1.9195024967193604
8	cs_sv	sv	sv_unigram_0.75k	750	2.6190483570098877	2.026533365249634
9	cs_sv	sv	sv_unigram_1.5k	1500	2.882889986038208	1.8612146377563477
10	cs_sv	sv	sv_unigram_3.0k	3000	2.457979679107666	1.7699251174926758
11	cs_sv	sv	sv_unigram_4.0k	4000	2.3484950065612793	1.7398505210876465
12	cs_sv	sv	sv_unigram_6.0k	6000	2.20053768157959	1.7809720039367676
13	cs_sv	sv	sv_unigram_8.0k	8000	2.0801753997802734	1.9306962490081787
