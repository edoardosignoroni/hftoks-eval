	tokenizer_type	vocab_size	freq@95%	avg_len	dataset
10	char	118	1	97.1026873767258	en_ga
23	char	123	1	107.824827416174	en_ga
25	unigram	500	3	42.835798816568	en_ga
1	bpe	500	3	39.5477071005917	en_ga
4	unigram	500	4	38.6881163708087	en_ga
14	bpe	500	2	43.7622041420118	en_ga
2	bpe	1000	5	32.1580374753452	en_ga
22	unigram	1000	24	35.5171351084813	en_ga
6	unigram	1000	23	31.383875739645	en_ga
18	bpe	1000	5	35.7681213017751	en_ga
20	unigram	2000	10	30.1209319526627	en_ga
16	bpe	2000	5	30.0181213017752	en_ga
12	unigram	2000	9	26.3431952662722	en_ga
8	bpe	2000	4	26.710798816568	en_ga
13	unigram	4000	4	26.5310650887574	en_ga
5	unigram	4000	3	23.0252712031558	en_ga
24	bpe	4000	3	25.9065581854043	en_ga
0	bpe	4000	2	22.8738905325444	en_ga
11	unigram	8000	1	21.4587031558185	en_ga
9	bpe	8000	1	20.4605522682446	en_ga
17	unigram	8000	1	24.5220660749507	en_ga
21	bpe	8000	1	23.1634615384615	en_ga
19	bpe	16000	1	21.4964250493097	en_ga
3	bpe	16000	1	19.0729783037475	en_ga
15	bpe	32000	1	20.8265532544379	en_ga
7	bpe	32000	1	18.7729289940828	en_ga
