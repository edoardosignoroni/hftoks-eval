{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "REFERENCE:\n",
    "https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "!!! BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data\n",
    "    > limit at 8k\n",
    "    > bacause of max number of unique forms in train data?\n",
    "    > clarify vocab_size vs merge operations. does sentencepiece uses vocab or merge for bpe implementation?\n",
    "    \n",
    "!!! in original paper tokenized with Sacremoses but it has no option to change voc_size\n",
    "    > sentencepiece has a bpe mode and char, test that and adapt functions and vocab_sizes to accomodate that\n",
    "        > sentencepiece implementation is good enough\n",
    "        \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final runs\n",
    "    > add incremental row index to df for storing\n",
    "    > check the correctness of frea@95 and avg_len stats\n",
    "\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./data/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./data/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "        \n",
    "        sp_model = spm.SentencePieceProcessor()\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        \n",
    "        #the revoming part gets the script stuck after a while. removing manually makes it work\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=paths,\n",
    "            model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "            vocab_size=vocab_size,\n",
    "            unk_id=2,\n",
    "            bos_id=-1,\n",
    "            eos_id=1,\n",
    "            pad_id=0,\n",
    "            model_type=self.model_type,\n",
    "            train_extremely_large_corpus=False\n",
    "        )\n",
    " \n",
    "        end = time.time()\n",
    "        \n",
    "        print(f'Training time: {end-start}')\n",
    "\n",
    "        print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "       \n",
    "    def make_freqs(self, lang, tokenizer, save_tokenized=False):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "        \n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(f'{tokenizer_path}')\n",
    "        \n",
    "        vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "        if save_tokenized == True:\n",
    "            if os.path.isfile(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                os.remove(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "        \n",
    "        freq = {}\n",
    "        with open(f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as f:\n",
    "            start = time.time()\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                toks = sp.encode_as_pieces(line)\n",
    "                for piece in toks:\n",
    "                    freq.setdefault(piece, 0)\n",
    "                    freq[piece] += 1\n",
    "            \n",
    "                if save_tokenized == True:\n",
    "                    with open(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                        print(toks, file=out)\n",
    "            end = time.time()\n",
    "        \n",
    "        freq = sorted(freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "        with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "            print(freq, file=out)\n",
    "\n",
    "        print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "        print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "\n",
    "    \n",
    "    def run(self\n",
    "            #plot=False\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        langs = [self.src_lang, self.tgt_lang]\n",
    "       \n",
    "        for lang in langs:\n",
    "            \n",
    "            if self.model_type=='char':\n",
    "                vocab_sizes = [self.count_chars(lang)]\n",
    "            elif self.model_type=='bpe':\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000, \n",
    "                               16000,\n",
    "                               32000, \n",
    "                               #48000, too big for en-ga\n",
    "                               #64000 too big for en-mr\n",
    "                               ]\n",
    "            elif self.model_type=='unigram':\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000\n",
    "                               ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                \n",
    "                    self.train_tokenizer(lang, size)\n",
    "                    tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                    self.make_freqs(lang, tokenizer_name, save_tokenized=True)\n",
    "         \n",
    "    def collect_stats(self, lang):\n",
    "        \n",
    "        tokenizers = os.listdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        df = pd.DataFrame(columns=['tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "            \n",
    "        for tokenizer in tokenizers:\n",
    "        \n",
    "            tokenizer_name = os.path.basename(tokenizer)\n",
    "            tokenized_text = open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'r')\n",
    "            freqs = ast.literal_eval(open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer_name}.freq').read())\n",
    "            freqs = list(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "            freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "            lines = tokenized_text.readlines()\n",
    "            \n",
    "            avg_len = 0\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.split(',')\n",
    "                avg_len += len(line)\n",
    "                \n",
    "            avg_len = avg_len/len(lines)\n",
    "\n",
    "            vocab_size = int(re.sub('\\D',\"\", tokenizer_name))*1000\n",
    "            \n",
    "            for row in df:\n",
    "                df.at[row[0], \"tokenizer\"] = tokenizer_name\n",
    "                df.at[row[0], \"vocab_size\"] = vocab_size\n",
    "                df.at[row[0], \"freq@95%\"] = freq_at_95\n",
    "                df.at[row[0], \"avg_len\"] = avg_len\n",
    "            \n",
    "        print(df.head())\n",
    "    \n",
    "    def plot(tokenizers):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ga_char = TokBuilder('en-ga',  model_type='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ga_char.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d385207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tokenizer vocab_size freq@95% avg_len\n",
      "t  en_unigram_2.0k      20000        9  2044.0\n",
      "v  en_unigram_2.0k      20000        9  2044.0\n",
      "f  en_unigram_2.0k      20000        9  2044.0\n",
      "a  en_unigram_2.0k      20000        9  2044.0\n"
     ]
    }
   ],
   "source": [
    "en_ga_char.collect_stats('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [str(x) for x in Path('./tokenizers/en_mr/').glob('**/*')]\n",
    "\n",
    "plot(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
