{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "REFERENCE:\n",
    "https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "!!! BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data\n",
    "    > limit at 8k\n",
    "    > bacause of max number of unique forms in train data?\n",
    "    > clarify vocab_size vs merge operations. does sentencepiece uses vocab or merge for bpe implementation?\n",
    "    \n",
    "!!! in original paper tokenized with Sacremoses but it has no option to change voc_size\n",
    "    > sentencepiece has a bpe mode and char, test that and adapt functions and vocab_sizes to accomodate that\n",
    "        > sentencepiece implementation is good enough\n",
    "        \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > check the correctness of freq@95 and avg_len stats\n",
    "\n",
    "!!! implement hft in class\n",
    "    > kind of works (does not give tokenized output), problems with ga pretokenized\n",
    "    > finish implementation, convert outputs to dict for implementation\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "    > grid plots, change df to include dataset, model, value\n",
    "\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./data/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./data/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0) #problems with pretokenize in loop\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            return (tokenizer_name, end-start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "        print(f'Training time: {end-start}')\n",
    "\n",
    "        print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "        return (tokenizer_name, end-start)\n",
    "       \n",
    "    def make_freqs(self, lang, tokenizer, save_tokenized=False):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            \n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                \n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = line[1].strip('\\n')\n",
    "            \n",
    "            train_path = f'./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            with open(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "               \n",
    "                cmd = f'python hftoks.py tokenize {tokenizer_path} {train_path} {out}'\n",
    "                toks = os.system(cmd)\n",
    "\n",
    "                print(toks, file=out)\n",
    "\n",
    "            end = time.time()\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "            return (tokenizer, end-start)\n",
    "            \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "            if save_tokenized == True:\n",
    "                if os.path.isfile(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "\n",
    "            freqs = {}\n",
    "            with open(f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as f:\n",
    "                start = time.time()\n",
    "                for line in f:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    for piece in toks:\n",
    "                        freqs.setdefault(piece, 0)\n",
    "                        freqs[piece] += 1\n",
    "\n",
    "                    if save_tokenized == True:\n",
    "                        with open(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()\n",
    "\n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "            return (tokenizer, end-start)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        train_times = []\n",
    "        token_times = []\n",
    "        \n",
    "        for lang in langs:\n",
    "            \n",
    "            if self.model_type=='char':\n",
    "                vocab_sizes = [self.count_chars(lang)]\n",
    "            elif self.model_type=='bpe': #merge operations\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000, \n",
    "                               16000,\n",
    "                               32000, \n",
    "                               #48000, too big for en-ga\n",
    "                               #64000 too big for en-mr\n",
    "                               ]\n",
    "            elif self.model_type=='unigram': #final vocabulary size\n",
    "                vocab_sizes = [500,\n",
    "                               750,\n",
    "                               1500,\n",
    "                               3000,\n",
    "                               4000,\n",
    "                               6000,\n",
    "                               8000\n",
    "                               ]\n",
    "            elif self.model_type=='hft': #final vocabulary size\n",
    "                vocab_sizes = [500,\n",
    "                               750,\n",
    "                               1500,\n",
    "                               3000,\n",
    "                               4000,\n",
    "                               6000,\n",
    "                               8000\n",
    "                               ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                \n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                    tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                    token_time = self.make_freqs(lang, tokenizer_name, save_tokenized=True)\n",
    "                    train_times.append(train_time)\n",
    "                    token_times.append(token_time)\n",
    "                    \n",
    "        print(train_times, token_times)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_mr bpe\n",
      "Training tokenizer for en with vocab_size of 500\n",
      "Training time: 0.5869386196136475\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.6232068538665771 with en_bpe_0.5k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 1000\n",
      "Training time: 0.49231934547424316\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.548546314239502 with en_bpe_1.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 2000\n",
      "Training time: 0.5844416618347168\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.8614680767059326 with en_bpe_2.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 4000\n",
      "Training time: 0.7667069435119629\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.9600892066955566 with en_bpe_4.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 8000\n",
      "Training time: 1.1233117580413818\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.9459633827209473 with en_bpe_8.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 16000\n",
      "Training time: 1.6011412143707275\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.9551966190338135 with en_bpe_16.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 32000\n",
      "Training time: 3.0579071044921875\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "en text tokenized in 1.9091155529022217 with en_bpe_32.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 500\n",
      "Training time: 0.8597695827484131\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 1.5398008823394775 with mr_bpe_0.5k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 1000\n",
      "Training time: 1.0626471042633057\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 1.5930445194244385 with mr_bpe_1.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 2000\n",
      "Training time: 1.3593037128448486\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 1.9620494842529297 with mr_bpe_2.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 4000\n",
      "Training time: 1.4215447902679443\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 2.0733585357666016 with mr_bpe_4.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 8000\n",
      "Training time: 2.1559698581695557\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 2.069995641708374 with mr_bpe_8.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 16000\n",
      "Training time: 4.4194769859313965\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 2.5521905422210693 with mr_bpe_16.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for mr with vocab_size of 32000\n",
      "Training time: 9.827463388442993\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "mr text tokenized in 2.4553117752075195 with mr_bpe_32.0k\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "[('en_bpe_0.5k', 0.5869386196136475), ('en_bpe_1.0k', 0.49231934547424316), ('en_bpe_2.0k', 0.5844416618347168), ('en_bpe_4.0k', 0.7667069435119629), ('en_bpe_8.0k', 1.1233117580413818), ('en_bpe_16.0k', 1.6011412143707275), ('en_bpe_32.0k', 3.0579071044921875), ('mr_bpe_0.5k', 0.8597695827484131), ('mr_bpe_1.0k', 1.0626471042633057), ('mr_bpe_2.0k', 1.3593037128448486), ('mr_bpe_4.0k', 1.4215447902679443), ('mr_bpe_8.0k', 2.1559698581695557), ('mr_bpe_16.0k', 4.4194769859313965), ('mr_bpe_32.0k', 9.827463388442993)] [('en_bpe_0.5k', 1.6232068538665771), ('en_bpe_1.0k', 1.548546314239502), ('en_bpe_2.0k', 1.8614680767059326), ('en_bpe_4.0k', 1.9600892066955566), ('en_bpe_8.0k', 1.9459633827209473), ('en_bpe_16.0k', 1.9551966190338135), ('en_bpe_32.0k', 1.9091155529022217), ('mr_bpe_0.5k', 1.5398008823394775), ('mr_bpe_1.0k', 1.5930445194244385), ('mr_bpe_2.0k', 1.9620494842529297), ('mr_bpe_4.0k', 2.0733585357666016), ('mr_bpe_8.0k', 2.069995641708374), ('mr_bpe_16.0k', 2.5521905422210693), ('mr_bpe_32.0k', 2.4553117752075195)]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            #'en_ga',\n",
    "           'en_mr'\n",
    "           ]\n",
    "model_types = [\n",
    "              #'char',\n",
    "              #'unigram',\n",
    "              'bpe',\n",
    "              #'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset,  model_type=model_type)\n",
    "        model.run()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = f'./data/{dataset}'\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in self.pair:\n",
    "            tokenizers = {} # tokenizer : ( freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "                if \"hft\" not in path:\n",
    "\n",
    "\n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    train = f'{self.dataset_dir}/train.{lang}'\n",
    "                    tokenized = f'{self.dataset_dir}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "        \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['tokenizer_type', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    freqs_path = paths[lang][tokenizer][0]\n",
    "                    tokenized_path = paths[lang][tokenizer][2]\n",
    "                    \n",
    "                    tokenized_text = open(tokenized_path, 'r')\n",
    "                    freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                    freqs = list(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "                    freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                    lines = tokenized_text.readlines()\n",
    "\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                    vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "                    \n",
    "                    if \"unigram\" in tokenizer:\n",
    "                        tokenizer_type = \"unigram\"\n",
    "                    elif \"bpe\" in tokenizer:\n",
    "                        tokenizer_type = \"bpe\"\n",
    "                    elif \"char\" in tokenizer:\n",
    "                        tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                    \n",
    "\n",
    "                    df.at[last_index, \"tokenizer_type\"] = tokenizer_type\n",
    "                    df.at[last_index, \"vocab_size\"] = vocab_size\n",
    "                    df.at[last_index, \"freq@95%\"] = freq_at_95\n",
    "                    df.at[last_index, \"avg_len\"] = avg_len\n",
    "                    last_index+=1\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open('./df_test.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n",
    "    \n",
    "    def plot(self, value):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \n",
    "        plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\n",
    "        do not matter\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.collect_stats()\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.lineplot(data=df,\n",
    "                    x=\"vocab_size\", y=value, hue=\"tokenizer_type\", style=\"tokenizer_type\",\n",
    "                    ci=None, markers=True, dashes=False, palette=\"tab10\", linewidth=2.5)\n",
    "        ax.set(title={self.dataset},\n",
    "                    xlabel=\"Vocabulary size\",\n",
    "                    ylabel=value,\n",
    "                    )\n",
    "        #ax.set(yscale='log')\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga')\n",
    "p2 = Plotter('en_mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240a206b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mPlotter.plot\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    returns plot\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    do not matter\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     sns\u001b[38;5;241m.\u001b[39mset_theme(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m     ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mlineplot(data\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m    101\u001b[0m                 x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39mvalue, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m                 ci\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, markers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dashes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab10\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mPlotter.collect_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(tokenized_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m freqs \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(\u001b[38;5;28mopen\u001b[39m(freqs_path)\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m---> 54\u001b[0m freqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     56\u001b[0m freq_at_95 \u001b[38;5;241m=\u001b[39m freqs[\u001b[38;5;28mint\u001b[39m((\u001b[38;5;28mlen\u001b[39m(freqs)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m95\u001b[39m)][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     58\u001b[0m lines \u001b[38;5;241m=\u001b[39m tokenized_text\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "p.plot('avg_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.plot('freq@95%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
