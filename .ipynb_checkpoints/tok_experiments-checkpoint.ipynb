{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "VVV BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data. Still\n",
    "    BPE uses final vocabulary size in sentence piece, even when another model cannot use that size\n",
    "            \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > I need:\n",
    "        > vocab_size\n",
    "        > length \n",
    "        > freq@95%\n",
    "    > check the correctness of freq@95 and avg_len stats DO THIS AND BE SURE!!!! DATA ARE STRANGE\n",
    "        > get percentiles of used tokens\n",
    "        > FREQS WERE ALL WRONG, RECOMPUTE (HOW?)\n",
    "            >for percs can use a sample if too big\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "    > grid plots, change df to include dataset, model, value\n",
    "    > must plot all things together\n",
    "\n",
    "!!! better self contained functions\n",
    "    > selective run to be passed in init\n",
    "    > separate make_freqs from tokenization\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type, data_path):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn {self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "   \n",
    "    def tokenize(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenize train for lang\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "        \n",
    "            train_path = f'./{self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            out = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'\n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            if os.path.isfile(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "            \n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as text:\n",
    "                \n",
    "                start = time.time()               \n",
    "                for line in text:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()   \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return(end-start)\n",
    "\n",
    "    def make_freqs(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                \n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = int(line[1].strip('\\n'))\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "            \n",
    "        else:\n",
    "            start=time.time()\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "            tokenized_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'\n",
    "        \n",
    "        \n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            toks = open(tokenized_path, 'r').readlines()\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "            \n",
    "            freqs = {}\n",
    "            \n",
    "            for line in toks:\n",
    "                line = ast.literal_eval(line)\n",
    "                for tok in line:\n",
    "                    #print (tok)\n",
    "                    if tok in vocabs:\n",
    "                        if tok in freqs.keys():\n",
    "                            freqs[tok] = freqs[tok]+1\n",
    "                        else:\n",
    "                            freqs[tok] = 1\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "\n",
    "            \"\"\"\n",
    "            freqs = {}\n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'r') as f:\n",
    "                for line in f:\n",
    "                    for tok in line:\n",
    "                        if tok in vocabs:\n",
    "\n",
    "\n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "            print(freqs, file=out)\n",
    "            \"\"\"\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "\n",
    "    def run(self, langs=None, vocab_sizes=None, train=True, tokenize=True, freqs=True, save_run=True):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        if not langs:\n",
    "            langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "        \n",
    "        for lang in langs:\n",
    "           \n",
    "            if not vocab_sizes:\n",
    "            \n",
    "                if self.model_type=='char':\n",
    "                    vocab_sizes = [self.count_chars(lang)]\n",
    "                elif self.model_type=='bpe': #merge operations\n",
    "                    vocab_sizes = [500,\n",
    "                                   1000,\n",
    "                                   2000,\n",
    "                                   4000,\n",
    "                                   8000, \n",
    "                                   16000,\n",
    "                                   32000, \n",
    "                                   #48000, too big for en-ga\n",
    "                                   #64000 too big for en-mr\n",
    "                                   ]\n",
    "                elif self.model_type=='unigram': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "                elif self.model_type=='hft': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "            \n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                if train:\n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                if tokenize:\n",
    "                    token_time = self.tokenize(lang, tokenizer_name)                   \n",
    "                if freqs: \n",
    "                    self.make_freqs(lang, tokenizer_name)\n",
    "                if save_run:\n",
    "                    row = {'dataset':self.pair, 'lang':lang, 'tokenizer':tokenizer_name, 'vocab_size':size, 'train':train_time, 'token':token_time}\n",
    "                    df = df.append(row, ignore_index=True)\n",
    "        if save_run:        \n",
    "            df.to_csv(f'./run_{time.time()}.csv', sep='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_hi char\n",
      "Made freqs for en_char_0.785k in 163.89026355743408\n",
      "Made freqs for hi_char_0.785k in 175.92868828773499\n",
      "en_hi unigram\n",
      "Made freqs for en_unigram_0.5k in 104.94966316223145\n",
      "Made freqs for en_unigram_0.75k in 117.92634892463684\n",
      "Made freqs for en_unigram_1.5k in 132.85157203674316\n",
      "Made freqs for en_unigram_3.0k in 161.04512786865234\n",
      "Made freqs for en_unigram_4.0k in 169.99364256858826\n",
      "Made freqs for en_unigram_6.0k in 214.092200756073\n",
      "Made freqs for en_unigram_8.0k in 241.60491633415222\n",
      "Made freqs for hi_unigram_0.5k in 103.74138808250427\n",
      "Made freqs for hi_unigram_0.75k in 106.5148503780365\n",
      "Made freqs for hi_unigram_1.5k in 122.69148111343384\n",
      "Made freqs for hi_unigram_3.0k in 158.46576380729675\n",
      "Made freqs for hi_unigram_4.0k in 181.24632143974304\n",
      "Made freqs for hi_unigram_6.0k in 215.98538827896118\n",
      "Made freqs for hi_unigram_8.0k in 249.22084498405457\n",
      "en_hi bpe\n",
      "Made freqs for en_bpe_0.5k in 114.98085117340088\n",
      "Made freqs for en_bpe_1.0k in 145.31648921966553\n",
      "Made freqs for en_bpe_2.0k in 194.85916757583618\n",
      "Made freqs for en_bpe_4.0k in 295.1648654937744\n",
      "Made freqs for en_bpe_8.0k in 464.14926958084106\n",
      "Made freqs for en_bpe_16.0k in 691.5987863540649\n",
      "Made freqs for en_bpe_32.0k in 1060.7612252235413\n",
      "Made freqs for hi_bpe_0.5k in 136.389568567276\n",
      "Made freqs for hi_bpe_1.0k in 168.18966555595398\n",
      "Made freqs for hi_bpe_2.0k in 222.811851978302\n",
      "Made freqs for hi_bpe_4.0k in 306.84823083877563\n",
      "Made freqs for hi_bpe_8.0k in 445.3655426502228\n",
      "Made freqs for hi_bpe_16.0k in 653.6103065013885\n",
      "Made freqs for hi_bpe_32.0k in 973.9702832698822\n",
      "en_hi hft\n",
      "Made freqs for en_hft_0.5k in 0.004786491394042969\n",
      "Made freqs for en_hft_0.75k in 0.0009713172912597656\n",
      "Made freqs for en_hft_1.5k in 0.0017905235290527344\n",
      "Made freqs for en_hft_3.0k in 0.0027425289154052734\n",
      "Made freqs for en_hft_4.0k in 0.003760099411010742\n",
      "Made freqs for en_hft_6.0k in 0.0053195953369140625\n",
      "Made freqs for en_hft_8.0k in 0.006765127182006836\n",
      "Made freqs for hi_hft_0.5k in 0.0007581710815429688\n",
      "Made freqs for hi_hft_0.75k in 0.0007755756378173828\n",
      "Made freqs for hi_hft_1.5k in 0.0015115737915039062\n",
      "Made freqs for hi_hft_3.0k in 0.002804994583129883\n",
      "Made freqs for hi_hft_4.0k in 0.0035276412963867188\n",
      "Made freqs for hi_hft_6.0k in 0.005530595779418945\n",
      "Made freqs for hi_hft_8.0k in 0.007005929946899414\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            #'en_mr',\n",
    "            #'en_ga',\n",
    "            \"en_hi\",\n",
    "            #\"lt_en\"\n",
    "           ]\n",
    "model_types = [\n",
    "              'char',\n",
    "              'unigram',\n",
    "              'bpe',\n",
    "              'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type, data_path='./data_big')\n",
    "        model.run(langs=['en', 'hi'], train=False, tokenize=False, freqs=True, save_run=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset, dataset_dir, just_tgt=False):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        self.just_tgt = just_tgt\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \n",
    "        langs = self.pair\n",
    "        if self.just_tgt:\n",
    "            langs = [langs[1]]\n",
    "       \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in langs:\n",
    "            tokenizers = {} # tokenizer : (freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "                \n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    \n",
    "                    if 'hft' in path:\n",
    "                        train = f'{self.dataset_dir}/{self.dataset}/train/tokenized/hft_pretokenized.{lang}'\n",
    "                    else:\n",
    "                        train = f'{self.dataset_dir}/{self.dataset}/train.{lang}'\n",
    "                    \n",
    "                    tokenized = f'{self.dataset_dir}/{self.dataset}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "    \n",
    "    \n",
    "    def collect_percs(self):\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', '0', '10', '100'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                 \n",
    "                if self.model_type == 'hft':    \n",
    "                    vocab = open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab', 'r')\n",
    "                    values = [int(line[1]) for line.split('\\t') in vocab.readlines()]\n",
    "                \n",
    "                else:\n",
    "                    vocab = \n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "                \n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "        \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                #freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "\n",
    "                percs = [0, 10, 100]\n",
    "                zero = 0\n",
    "                ten = 0\n",
    "                hundr = 0\n",
    "                \n",
    "                for i in percs:\n",
    "                    n = 0\n",
    "                    for value in freqs.values():\n",
    "                        if int(value) <= i:\n",
    "                             n += 1\n",
    "                    perc = (n/len(freqs))*100            \n",
    "                    \n",
    "                    if i == 0:\n",
    "                        zero = perc\n",
    "                    elif i == 10:\n",
    "                        ten = perc\n",
    "                    elif i == 100:\n",
    "                        hundr = perc\n",
    "\n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                   \"lang\" : lang,                  \n",
    "                   \"tokenizer\" : tokenizer,\n",
    "                   \"vocab_size\" : vocab_size,\n",
    "                   \"0\" : zero,\n",
    "                   \"10\" : ten,\n",
    "                    \"100\" : hundr}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./{self.dataset}_percs.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "    \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    \n",
    "                freqs_path = paths[lang][tokenizer][0]\n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "   \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "                \n",
    "                freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                lines = tokenized_text.readlines()\n",
    "\n",
    "                avg_len = 0\n",
    "\n",
    "                for line in lines:\n",
    "                    line = line.split(',')\n",
    "                    avg_len += len(line)\n",
    "\n",
    "                avg_len = avg_len/len(lines)\n",
    "\n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "                if \"unigram\" in tokenizer:\n",
    "                    tokenizer_type = \"unigram\"\n",
    "                elif \"bpe\" in tokenizer:\n",
    "                    tokenizer_type = \"bpe\"\n",
    "                elif \"char\" in tokenizer:\n",
    "                    tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                elif \"hft\" in tokenizer:\n",
    "                    tokenizer_type = \"hft\"\n",
    "                    \n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                       \"lang\" : lang,                  \n",
    "                       \"tokenizer\" : tokenizer_type,\n",
    "                       \"vocab_size\" : vocab_size,\n",
    "                       \"freq@95%\" : freq_at_95,\n",
    "                       \"avg_len\" : avg_len}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./{self.dataset}.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga', './data')\n",
    "p2 = Plotter('en_mr', './data')\n",
    "p3 = Plotter('en_hi', './data_big')\n",
    "p4 = Plotter('lt_en', './data_big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.collect_percs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.collect_percs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f73cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.collect_percs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e90de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4.collect_percs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4696e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
