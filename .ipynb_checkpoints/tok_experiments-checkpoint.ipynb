{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, model_type):\n",
    "       \n",
    "        self.lang = lang\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    def make_batches(self):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./data/{self.lang}/train/train.{self.lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./data/{self.lang}/train/train_{file_count}.{self.lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_batches(self):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches()\n",
    "        paths = [str(x) for x in Path(f'./data/{self.lang}/train').glob(f'**/*.{self.lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \n",
    "        if hft, must pretokenize beforehand\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training {self.model_type} tokenizer for {self.lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{self.lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_batches()\n",
    "        \n",
    "        tokenizer_path = f'./data/{self.lang}/tokenizers/{tokenizer_name}'\n",
    "        \n",
    "        #if not os.path.isdir(f'./data/{self.lang}/tokenizers/'):\n",
    "         #   os.mkdir(f'./data/{self.lang}/tokenizers/')\n",
    "            \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            cmd0 = f'./pretokenize ./data/{self.lang}/train/train.{self.lang} > ./data/{self.lang}/train/train_hft_pretokenized.{self.lang}' \n",
    "            cmd1 = f'./hftoks.py learn ./data/{self.lang}/train/train_hft_pretokenized.{self.lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            #print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            manychars = ['ja']\n",
    "            \n",
    "            charcover = 1.0\n",
    "            \n",
    "            if self.lang in manychars:\n",
    "                charcover = 0.98\n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "            \n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                character_coverage=charcover,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "            #print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "   \n",
    "    def tokenize(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenize train for lang with tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        out = f'./data/{self.lang}/tokenized/train_toks_{tokenizer}.{self.lang}'\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./data/{self.lang}/tokenizers//{tokenizer}/{tokenizer}.vocab'\n",
    "        \n",
    "            train_path = f'./data/{self.lang}/train/train_hft_pretokenized.{self.lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            #print(f'{self.lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./data/{self.lang}/tokenizers/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            if os.path.isfile(out):\n",
    "                    os.remove(out)\n",
    "            \n",
    "            with open(f'./data/{self.lang}/train/train.{self.lang}', 'r') as text:\n",
    "                \n",
    "                start = time.time()               \n",
    "                for line in text:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    with open(out, 'a+') as outtoks:\n",
    "                            print(toks, file=outtoks)\n",
    "                end = time.time()   \n",
    "            #print(f'{self.lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return(end-start)\n",
    "\n",
    "    def make_freqs(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        start=time.time()\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./data/{self.lang}/tokenizers/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            vocab = open(tokenizer_path, 'r').readlines()\n",
    "            vocabs = []\n",
    "\n",
    "            for line in vocab:\n",
    "                line=line.split(\"\\t\")\n",
    "                vocabs.append(line[0])\n",
    "\n",
    "        else:\n",
    "            tokenizer_path = f'./data/{self.lang}/tokenizers/{tokenizer}/{tokenizer}.model'\n",
    "            \n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "        tokenized_path = f'./data/{self.lang}/tokenized/train_toks_{tokenizer}.{self.lang}'\n",
    "        toks = open(tokenized_path, 'r').readlines()\n",
    "        \n",
    "        freqs = {}\n",
    "\n",
    "        for line in toks:\n",
    "            if model_type == 'hft':\n",
    "                line = line.split(' ')\n",
    "            else:\n",
    "                line = ast.literal_eval(line)\n",
    "            for tok in line:\n",
    "                if tok in vocabs:\n",
    "                    if tok in freqs.keys():\n",
    "                        freqs[tok] += 1\n",
    "                    else:\n",
    "                        freqs[tok] = 1\n",
    "\n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./data/{self.lang}/tokenizers/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "        \n",
    "        end = time.time()\n",
    "        #print(f'Collected freq for {tokenizer} in {end-start}')\n",
    "            \n",
    "    def run(self, vocab_sizes=None, train=True, tokenize=True, freqs=True, save_run=True):\n",
    "        \"\"\"\n",
    "        Runs the other functions, returns train time and tokenization time\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame(columns=['lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "\n",
    "        if not vocab_sizes:\n",
    "            if self.lang == 'ja':\n",
    "                vocab_sizes = [\n",
    "                           #500,\n",
    "                           750,\n",
    "                           1500,\n",
    "                           3000,\n",
    "                           4000,\n",
    "                           6000,\n",
    "                           8000\n",
    "                               ]\n",
    "            elif self.lang == 'quc':\n",
    "                vocab_sizes = [\n",
    "                           500,\n",
    "                           750,\n",
    "                           1500,\n",
    "                           3000,\n",
    "                           4000,\n",
    "                           #6000,\n",
    "                           #8000\n",
    "                               ]\n",
    "            \n",
    "            else:\n",
    "                vocab_sizes = [\n",
    "                               500,\n",
    "                               750,\n",
    "                               1500,\n",
    "                               3000,\n",
    "                               4000,\n",
    "                               6000,\n",
    "                               8000\n",
    "                                   ]\n",
    "           \n",
    "        for size in vocab_sizes:\n",
    "            tokenizer_name = f'{self.lang}_{self.model_type}_{size/1000}k'\n",
    "            train_time = 0\n",
    "            token_time = 0\n",
    "            if train:\n",
    "                train_time = self.train_tokenizer(size)\n",
    "            if tokenize:\n",
    "                token_time = self.tokenize(tokenizer_name)\n",
    "            if freqs:\n",
    "                self.make_freqs(tokenizer_name)\n",
    "            if save_run:\n",
    "                df.loc[len(df)]=[self.lang, tokenizer_name, size, train_time, token_time]\n",
    "        if save_run:        \n",
    "            df.to_csv(f'./stats/{self.lang}_time.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d439bfb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cannot run all together bacause jupyter saves outputs checkpoints, stalling the console. Run one dataset at a time,\n",
    "one model at a time, or pass one lang.\n",
    "\"\"\"\n",
    "\n",
    "datasets = [\n",
    "            #'am', V\n",
    "            #'ar', V\n",
    "            #'chr', V\n",
    "            #'cs', V\n",
    "            #'en',V\n",
    "            #'fi', V\n",
    "            #'ga',V\n",
    "            #'hi',V\n",
    "            #'it',V\n",
    "            #'ja',\n",
    "            'jak',\n",
    "            #'lt',V\n",
    "            #'mr',V\n",
    "            #'my',\n",
    "            #'ojb',\n",
    "            #'sv',\n",
    "            #'syr',\n",
    "            #'zu',\n",
    "            \n",
    "            ]\n",
    "model_types = [\n",
    "              'unigram',\n",
    "              'bpe',\n",
    "              'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type)\n",
    "        model.run(\n",
    "                  #vocab_sizes = [3000],\n",
    "                  #train=False,\n",
    "                  #tokenize=False,\n",
    "                  #freqs=False,\n",
    "                  #save_run=False\n",
    "                 )\n",
    "        clear_output(wait=True)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    \"\"\"\n",
    "    Called Plotter for previous versions, DOES NOT PLOT (use other notebook). Generates .csv with statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.tokenizers_dir = f'./data/{self.lang}/tokenizers/'\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary of paths of relevant files\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        tokenizers = {} # tokenizer : (freqs, train, tokenized)\n",
    "        tokenizers_paths = [path for path in os.listdir(f'./data/{self.lang}/tokenizers')]\n",
    "\n",
    "        for path in tokenizers_paths:\n",
    "                tokenizer_name = os.path.basename(path)\n",
    "                freqs = f'./data/{self.lang}/tokenizers/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "\n",
    "                if 'hft' in path:\n",
    "                    train = f'./data/{self.lang}/train/tokenized/train_hft_pretokenized.{self.lang}'\n",
    "                else:\n",
    "                    train = f'./data/{self.lang}/train/tokenized/train.{self.lang}'\n",
    "\n",
    "                tokenized = f'./data/{self.lang}/tokenized/train_toks_{tokenizer_name}.{self.lang}'\n",
    "\n",
    "                tokenizers[path] = (freqs, train, tokenized)\n",
    "\n",
    "        paths[self.lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "    \n",
    "    def metric(self, lang, size, model_type):\n",
    "        \n",
    "        size = size/1000\n",
    "\n",
    "        f_m = open(f'./data/{self.lang}/tokenizers/{self.lang}_{model_type}_{size}k/{self.lang}_{model_type}_{size}k.freq', 'r').read()\n",
    "        f_m = ast.literal_eval(f_m)\n",
    "\n",
    "        i = 1\n",
    "\n",
    "        weighted_m_val = 0\n",
    "\n",
    "        m_95_len = int((len(f_m)/100)*95)\n",
    "\n",
    "        for val in f_m.keys():\n",
    "\n",
    "            weighted_m_val += int(f_m[val])*i\n",
    "            \n",
    "            if i > m_95_len:\n",
    "                break\n",
    "           \n",
    "            i += 1\n",
    "\n",
    "        weighted_m_val = weighted_m_val/sum(range(i))\n",
    "        \n",
    "        return weighted_m_val\n",
    "\n",
    "    \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        Collects the stats and generates a .csv\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len','weighted'])\n",
    "\n",
    "        for tokenizer in paths[self.lang].keys():\n",
    "\n",
    "            freqs_path = paths[self.lang][tokenizer][0]\n",
    "            tokenized_path = paths[self.lang][tokenizer][2]\n",
    "\n",
    "            tokenized_text = open(tokenized_path, 'r')\n",
    "\n",
    "            freqs = ast.literal_eval(open(freqs_path).read())\n",
    "            freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "\n",
    "            index = math.floor((len(freqs)/100)*95)\n",
    "            \n",
    "            freq_at_95 = freqs[index][1]\n",
    "\n",
    "            lines = tokenized_text.readlines()\n",
    "\n",
    "            if 'hft' in tokenizer:\n",
    "                avg_len = 0\n",
    "\n",
    "                for line in lines:\n",
    "                    line = line.split(' ')\n",
    "                    avg_len += len(line)\n",
    "\n",
    "                avg_len = avg_len/len(lines)\n",
    "\n",
    "            else:\n",
    "                avg_len = 0\n",
    "\n",
    "                for line in lines:\n",
    "                    line = line.split(',')\n",
    "                    avg_len += len(line)\n",
    "\n",
    "                avg_len = avg_len/len(lines)\n",
    "\n",
    "            vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "            if \"unigram\" in tokenizer:\n",
    "                tokenizer_type = \"unigram\"\n",
    "            elif \"bpe\" in tokenizer:\n",
    "                tokenizer_type = \"bpe\"\n",
    "            elif \"hft\" in tokenizer:\n",
    "                tokenizer_type = \"hft\"\n",
    "\n",
    "            weighted = self.metric(self.lang, vocab_size, tokenizer_type)\n",
    "\n",
    "            df.loc[len(df)]=[self.lang, tokenizer_type, vocab_size, freq_at_95, avg_len, weighted]\n",
    "\n",
    "        df = df.sort_values(by=\"tokenizer\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./stats/{self.lang}.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "240a206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "ar\n",
      "chr\n",
      "cs\n",
      "en\n",
      "ja\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            'am',\n",
    "            'ar',\n",
    "            'chr',\n",
    "            'cs',\n",
    "            'en',\n",
    "            #'fi',\n",
    "            #'ga',\n",
    "            #'hi',\n",
    "            #'it',\n",
    "            'ja',\n",
    "            #'lt',\n",
    "            #'mr',\n",
    "            #'my',\n",
    "            #'sv',\n",
    "            #'syr',\n",
    "            #'zu',\n",
    "            #'ojb'\n",
    "            #'jak'\n",
    "            ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    Plotter(dataset).collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    TBA, future work\n",
    "    \n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def big_to_small(pair, size):\n",
    "    \"\"\"\n",
    "    generates a sample of a bigger file according to language pair and size\n",
    "    \"\"\"\n",
    "    random.seed(20220713)\n",
    "    \n",
    "    pair = pair.split('_')\n",
    "\n",
    "    big_path1 = f'./data_big/{pair[0]}_{pair[1]}/train.{pair[0]}'\n",
    "    big_file1 = open(big_path1, 'r').readlines()\n",
    "    small_path1 = f'./data/{pair[0]}_{pair[1]}/train.{pair[0]}'\n",
    "    \n",
    "    sample = random.sample(range(len(big_file1)), size)\n",
    "    \n",
    "    big_path2 = f'./data_big/{pair[0]}_{pair[1]}/train.{pair[1]}'\n",
    "    big_file2 = open(big_path2, 'r').readlines()\n",
    "    small_path2 = f'./data/{pair[0]}_{pair[1]}/train.{pair[1]}'\n",
    "    \n",
    "    #os.mkdir(f'./data/{pair[0]}_{pair[1]}')\n",
    "\n",
    "    with open(small_path1, 'w+') as small_file1:\n",
    "        for i in sample:\n",
    "            print(big_file1[i].strip('\\n'), file=small_file1)\n",
    "    \n",
    "    with open(small_path2, 'w+') as small_file2:\n",
    "        for i in sample:\n",
    "            print(big_file2[i].strip('\\n'), file=small_file2) \n",
    "\n",
    "def xml_to_raw(in_file, out_file):\n",
    "    \"\"\"\n",
    "    generates a raw txt from a xml\n",
    "    \"\"\"\n",
    "    \n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    with open(out_file, 'w+') as out:\n",
    "        for child in root.iter('s'):\n",
    "            \n",
    "                print(child.text, file=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b10ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_to_raw('./data/jak/train/Jakalteko-NT.xml', f'./data/jak/train/train.jak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b36c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
