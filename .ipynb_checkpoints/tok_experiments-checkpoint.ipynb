{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "VVV BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data. Still\n",
    "    BPE uses final vocabulary size in sentence piece, even when another model cannot use that size\n",
    "            \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > I need:\n",
    "        > vocab_size\n",
    "        > length \n",
    "        > freq@95%\n",
    "    > check the correctness of freq@95 and avg_len stats DO THIS AND BE SURE!!!! DATA ARE STRANGE\n",
    "        > get percentiles of used tokens\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "    > grid plots, change df to include dataset, model, value\n",
    "    > must plot all things together\n",
    "\n",
    "!!! better self contained functions\n",
    "    > selective run to be passed in init\n",
    "    > separate make_freqs from tokenization\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type, data_path):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            #cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn {self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            #os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False,\n",
    "                minloglevel=100\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "            print(f'Training time: {end-start}')\n",
    "            return (end-start)\n",
    "   \n",
    "    def tokenize(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenize train for lang\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "        \n",
    "            train_path = f'./{self.data_path}/{self.pair}/train/tokenized/hft_pretokenized.{lang}'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            out = f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'\n",
    "            cmd = f'python3 hftoks.py tokenize {tokenizer_path} <{train_path} > {out}'\n",
    "            os.system(cmd)\n",
    "\n",
    "            end = time.time()\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return (end-start)\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "            \n",
    "            if os.path.isfile(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "            \n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as text:\n",
    "                \n",
    "                start = time.time()               \n",
    "                for line in text:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()   \n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            return(end-start)\n",
    "\n",
    "    def make_freqs(self, lang, tokenizer):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            freqs_file = open(tokenizer_path, 'r')\n",
    "            freqs = {}\n",
    "            for line in freqs_file.readlines():\n",
    "                \n",
    "                line = line.split('\\t')\n",
    "                freqs[line[0].strip(' ')] = int(line[1].strip('\\n'))\n",
    "            \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "            \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "            \n",
    "            start=time.time()\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "            freqs = {}\n",
    "            with open(f'./{self.data_path}/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'r') as f:\n",
    "                for line in f:\n",
    "                    for piece in line:\n",
    "                        freqs.setdefault(piece, 0)\n",
    "                        freqs[piece] += 1\n",
    "                        \n",
    "            freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freqs, file=out)\n",
    "            \n",
    "            end=time.time()\n",
    "            print(f\"Made freqs for {tokenizer} in {end-start}\")\n",
    "\n",
    "    def run(self, langs=None, vocab_sizes=None, train=True, tokenize=True, freqs=True):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'train', 'token'])\n",
    "        if not langs:\n",
    "            langs = [self.src_lang, self.tgt_lang]\n",
    "        \n",
    "        for lang in langs:\n",
    "           \n",
    "            if not vocab_sizes:\n",
    "            \n",
    "                if self.model_type=='char':\n",
    "                    vocab_sizes = [self.count_chars(lang)]\n",
    "                elif self.model_type=='bpe': #merge operations\n",
    "                    vocab_sizes = [500,\n",
    "                                   1000,\n",
    "                                   2000,\n",
    "                                   4000,\n",
    "                                   8000, \n",
    "                                   16000,\n",
    "                                   32000, \n",
    "                                   #48000, too big for en-ga\n",
    "                                   #64000 too big for en-mr\n",
    "                                   ]\n",
    "                elif self.model_type=='unigram': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "                elif self.model_type=='hft': #final vocabulary size\n",
    "                    vocab_sizes = [500,\n",
    "                                   750,\n",
    "                                   1500,\n",
    "                                   3000,\n",
    "                                   4000,\n",
    "                                   6000,\n",
    "                                   8000\n",
    "                                   ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                if train:\n",
    "                    train_time = self.train_tokenizer(lang, size)\n",
    "                if tokenize:\n",
    "                    token_time = self.tokenize(lang, tokenizer_name)                   \n",
    "                if freqs: \n",
    "                    self.make_freqs(lang, tokenizer_name)\n",
    "                \n",
    "                row = {'dataset':self.pair, 'lang':lang, 'tokenizer':tokenizer_name, 'vocab_size':size, 'train':train_time, 'token':token_time}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "                \n",
    "        df.to_csv(f'./run_{time.time()}.csv', sep='\\t')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lt_en hft\n",
      "Training tokenizer for lt with vocab_size of 500\n",
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "Training time: 65.73120403289795\n",
      "lt text tokenized in 198.1664822101593 with lt_hft_0.5k\n",
      "Made freqs for lt_hft_0.5k in 0.001186370849609375\n",
      "Training tokenizer for lt with vocab_size of 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "5 634 24265 [('ğ‹‡Â¦sÄ…jung', 43293), ('Â¦pirmininkeÂ¦', 36737), ('Â¦todÄ—lÂ¦', 33717), ('imasÂ¦', 32930), ('Â¦mÅ«sÅ³Â¦', 32847)]\n",
      "6 710 19297 [('Â¦20', 33039), ('nink', 28363), ('Â¦eu', 28090), ('ğ‹‡Â¦e', 27025), ('Â¦sieki', 26076)]\n",
      "7 786 16304 [('aiÂ¦', 25802), ('Â¦dis', 23784), ('Â¦reikal', 23608), ('Â¦sud', 22800), ('ğ‹‡Â¦europ', 22254)]\n",
      "Training time: 88.39507412910461\n",
      "lt text tokenized in 201.43742966651917 with lt_hft_0.75k\n",
      "Made freqs for lt_hft_0.75k in 0.0011076927185058594\n",
      "Training tokenizer for lt with vocab_size of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "5 634 24265 [('ğ‹‡Â¦sÄ…jung', 43293), ('Â¦pirmininkeÂ¦', 36737), ('Â¦todÄ—lÂ¦', 33717), ('imasÂ¦', 32930), ('Â¦mÅ«sÅ³Â¦', 32847)]\n",
      "6 710 19297 [('Â¦20', 33039), ('nink', 28363), ('Â¦eu', 28090), ('ğ‹‡Â¦e', 27025), ('Â¦sieki', 26076)]\n",
      "7 786 16304 [('aiÂ¦', 25802), ('Â¦dis', 23784), ('Â¦reikal', 23608), ('Â¦sud', 22800), ('ğ‹‡Â¦europ', 22254)]\n",
      "8 858 13804 [('dÄ—lÂ¦', 19555), ('Â¦gali', 18585), ('Â¦ypaÄÂ¦', 18379), ('Â¦politik', 16854), ('Â¦min', 16746)]\n",
      "9 929 12044 [('Â¦tu', 15793), ('Â¦bendradarbi', 14730), ('ğ‹‡Â¦v', 14586), ('Â¦vad', 13984), ('Â¦tarptau', 13939)]\n",
      "10 996 10225 [('Â¦aplink', 14585), ('Â¦sektor', 12918), ('iniÅ³Â¦', 12726), ('Â¦ga', 12320), ('Â¦arbaÂ¦', 12311)]\n",
      "11 1068 9275 [('Â¦mÄ—', 13288), ('ğ‹‡Â¦m', 11491), ('tinÄ—sÂ¦', 10921), ('Â¦taisykl', 10826), ('yn', 10258)]\n",
      "12 1138 8352 [('yr', 12244), ('Â¦kit', 11173), ('Â¦patvirtin', 11031), ('Â¦pabrÄ—Å¾', 10450), ('Â¦social', 10109)]\n",
      "13 1207 7551 [('Â¦inter', 9970), ('ej', 9949), ('Â¦transp', 9182), ('bÅ«', 9109), ('Â¦Å¾e', 8650)]\n",
      "14 1268 6916 [('gu', 9980), ('Â¦rez', 9080), ('Â¦dv', 8474), ('Â¦iniciatyv', 7752), ('ulta', 7713)]\n",
      "15 1333 6367 [('urop', 12021), ('Â¦euro', 10999), ('ğ‹‡Â¦e', 10860), ('Â¦kar', 9253), ('vej', 8009)]\n",
      "16 1405 5727 [('Â¦fr', 9755), ('Â¦rezulta', 7972), ('Ä—ja', 6727), ('Â¦atÅ¾vilgiuÂ¦', 6674), ('Â¦nacional', 6527)]\n",
      "17 1472 5281 [('Â¦poveik', 7899), ('gosÂ¦', 6528), ('Ä—mes', 5988), ('Â¦koleg', 5825), ('ğ‹‡Â¦sÄ…jungojeÂ¦', 5726)]\n",
      "18 1548 4978 [('ganiz', 8704), ('Â¦orga', 8515), ('Â¦svarst', 6433), ('taik', 6169), ('Â¦pastan', 6118)]\n",
      "Training time: 176.7579209804535\n",
      "lt text tokenized in 213.65005660057068 with lt_hft_1.5k\n",
      "Made freqs for lt_hft_1.5k in 0.0032033920288085938\n",
      "Training tokenizer for lt with vocab_size of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "5 634 24265 [('ğ‹‡Â¦sÄ…jung', 43293), ('Â¦pirmininkeÂ¦', 36737), ('Â¦todÄ—lÂ¦', 33717), ('imasÂ¦', 32930), ('Â¦mÅ«sÅ³Â¦', 32847)]\n",
      "6 710 19297 [('Â¦20', 33039), ('nink', 28363), ('Â¦eu', 28090), ('ğ‹‡Â¦e', 27025), ('Â¦sieki', 26076)]\n",
      "7 786 16304 [('aiÂ¦', 25802), ('Â¦dis', 23784), ('Â¦reikal', 23608), ('Â¦sud', 22800), ('ğ‹‡Â¦europ', 22254)]\n",
      "8 858 13804 [('dÄ—lÂ¦', 19555), ('Â¦gali', 18585), ('Â¦ypaÄÂ¦', 18379), ('Â¦politik', 16854), ('Â¦min', 16746)]\n",
      "9 929 12044 [('Â¦tu', 15793), ('Â¦bendradarbi', 14730), ('ğ‹‡Â¦v', 14586), ('Â¦vad', 13984), ('Â¦tarptau', 13939)]\n",
      "10 996 10225 [('Â¦aplink', 14585), ('Â¦sektor', 12918), ('iniÅ³Â¦', 12726), ('Â¦ga', 12320), ('Â¦arbaÂ¦', 12311)]\n",
      "11 1068 9275 [('Â¦mÄ—', 13288), ('ğ‹‡Â¦m', 11491), ('tinÄ—sÂ¦', 10921), ('Â¦taisykl', 10826), ('yn', 10258)]\n",
      "12 1138 8352 [('yr', 12244), ('Â¦kit', 11173), ('Â¦patvirtin', 11031), ('Â¦pabrÄ—Å¾', 10450), ('Â¦social', 10109)]\n",
      "13 1207 7551 [('Â¦inter', 9970), ('ej', 9949), ('Â¦transp', 9182), ('bÅ«', 9109), ('Â¦Å¾e', 8650)]\n",
      "14 1268 6916 [('gu', 9980), ('Â¦rez', 9080), ('Â¦dv', 8474), ('Â¦iniciatyv', 7752), ('ulta', 7713)]\n",
      "15 1333 6367 [('urop', 12021), ('Â¦euro', 10999), ('ğ‹‡Â¦e', 10860), ('Â¦kar', 9253), ('vej', 8009)]\n",
      "16 1405 5727 [('Â¦fr', 9755), ('Â¦rezulta', 7972), ('Ä—ja', 6727), ('Â¦atÅ¾vilgiuÂ¦', 6674), ('Â¦nacional', 6527)]\n",
      "17 1472 5281 [('Â¦poveik', 7899), ('gosÂ¦', 6528), ('Ä—mes', 5988), ('Â¦koleg', 5825), ('ğ‹‡Â¦sÄ…jungojeÂ¦', 5726)]\n",
      "18 1548 4978 [('ganiz', 8704), ('Â¦orga', 8515), ('Â¦svarst', 6433), ('taik', 6169), ('Â¦pastan', 6118)]\n",
      "19 1615 4691 [('rÄ—', 9154), ('Â¦organiz', 8376), ('Ä—Å¾', 7698), ('Â¦kole', 7437), ('gaÂ¦', 5274)]\n",
      "20 1685 4376 [('Â¦susid', 6598), ('Â¦priorite', 5533), ('Â¦stabil', 4934), ('Â¦pasaul', 4872), ('Â¦spe', 4868)]\n",
      "21 1755 4077 [('brÄ—Å¾', 7365), ('prÄ™', 6840), ('Ä™stiÂ¦', 6390), ('naÅ¡', 5880), ('Â¦pastar', 4779)]\n",
      "22 1835 3805 [('Â¦sut', 5198), ('ğ‹‡Â¦ru', 4653), ('lyg', 4209), ('Â¦elektr', 4121), ('Â¦vaid', 4111)]\n",
      "23 1912 3576 [('ekl', 6460), ('Â¦iÅ¡t', 5000), ('Â¦lÄ—', 4776), ('Ä—Å¡', 4299), ('tek', 3877)]\n",
      "24 1972 3366 [('uomen', 5601), ('anc', 4730), ('Â¦supra', 4398), ('Â¦teritor', 4350), ('timiÂ¦', 4312)]\n",
      "25 2035 3187 [('Â¦atk', 5107), ('Â¦trÅ«k', 3758), ('ğ‹‡Â¦prancÅ«', 3552), ('Â¦iÅ¡tekl', 3515), ('Â¦mechaniz', 3482)]\n",
      "26 2108 3007 [('Â¦daugel', 4006), ('iuiÂ¦', 3628), ('Â¦duomen', 3458), ('aup', 3188), ('veiktiÂ¦', 3186)]\n",
      "27 2183 2852 [('Â¦nus', 4602), ('Â¦ilgalaik', 3405), ('ğŠ£Â¦plÂ¦ğŠ¼', 3160), ('Â¦ypat', 3120), ('ingÅ³Â¦', 3081)]\n",
      "28 2247 2709 [('ytosÂ¦', 3471), ('pen', 3354), ('Â¦id', 3060), ('Â¦prim', 2926), ('vykoÂ¦', 2851)]\n",
      "29 2316 2569 [('bl', 3191), ('els', 2840), ('intasÂ¦', 2796), ('ğŠ£Â¦s', 2707), ('Â¦pasir', 2707)]\n",
      "30 2394 2437 [('Â¦pal', 3846), ('Â¦prot', 3577), ('Â¦istor', 3210), ('Â¦fr', 2905), ('Â¦pon', 2864)]\n",
      "31 2463 2323 [('ac', 3004), ('Â¦mlrdÂ¦', 2551), ('Â¦kapit', 2478), ('eitiÂ¦', 2469), ('Â¦kl', 2453)]\n",
      "32 2535 2233 [('Â¦pi', 3885), ('Â¦trans', 3382), ('Â¦dir', 3205), ('Â¦didÅ¾', 2750), ('Â¦skurd', 2585)]\n",
      "33 2607 2145 [('Â¦rizik', 3776), ('lank', 3466), ('Â¦rekomenda', 2995), ('Â¦gyvyb', 2347), ('Â¦pala', 2295)]\n",
      "34 2672 2058 [('ktyv', 3958), ('ğ‹‡Â¦valst', 2678), ('..', 2518), ('Â¦Å¾iniasklaid', 2184), ('Â¦taigiÂ¦', 2173)]\n",
      "35 2744 1973 [('Â¦diskrimin', 2571), ('Â¦civil', 2552), ('bil', 2461), ('iasiÂ¦', 2404), ('Â¦kriter', 2395)]\n",
      "36 2811 1895 [('Â¦situ', 2672), ('..', 2518), ('dÅ¾iaÂ¦', 2126), ('tinÄ—msÂ¦', 2080), ('tern', 2032)]\n",
      "37 2885 1826 [('Â¦sava', 3301), ('ğ‹‡Â¦vokie', 2785), ('Â¦c', 2502), ('Â¦atnauj', 2364), ('Â¦kitiÂ¦', 2112)]\n",
      "38 2959 1757 [('..', 2518), ('umusÂ¦', 1831), ('Â¦nepaka', 1825), ('Â¦sutart', 1824), ('Â¦naf', 1821)]\n",
      "39 3036 1698 [('Â¦iÅ¡t', 3111), ('irÅ«p', 2295), ('Â¦biudÅ¾e', 2149), ('Â¦nepritar', 1970), ('Â¦reg', 1895)]\n",
      "Training time: 917.7200179100037\n",
      "lt text tokenized in 248.20147275924683 with lt_hft_3.0k\n",
      "Made freqs for lt_hft_3.0k in 0.002857208251953125\n",
      "Training tokenizer for lt with vocab_size of 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "5 634 24265 [('ğ‹‡Â¦sÄ…jung', 43293), ('Â¦pirmininkeÂ¦', 36737), ('Â¦todÄ—lÂ¦', 33717), ('imasÂ¦', 32930), ('Â¦mÅ«sÅ³Â¦', 32847)]\n",
      "6 710 19297 [('Â¦20', 33039), ('nink', 28363), ('Â¦eu', 28090), ('ğ‹‡Â¦e', 27025), ('Â¦sieki', 26076)]\n",
      "7 786 16304 [('aiÂ¦', 25802), ('Â¦dis', 23784), ('Â¦reikal', 23608), ('Â¦sud', 22800), ('ğ‹‡Â¦europ', 22254)]\n",
      "8 858 13804 [('dÄ—lÂ¦', 19555), ('Â¦gali', 18585), ('Â¦ypaÄÂ¦', 18379), ('Â¦politik', 16854), ('Â¦min', 16746)]\n",
      "9 929 12044 [('Â¦tu', 15793), ('Â¦bendradarbi', 14730), ('ğ‹‡Â¦v', 14586), ('Â¦vad', 13984), ('Â¦tarptau', 13939)]\n",
      "10 996 10225 [('Â¦aplink', 14585), ('Â¦sektor', 12918), ('iniÅ³Â¦', 12726), ('Â¦ga', 12320), ('Â¦arbaÂ¦', 12311)]\n",
      "11 1068 9275 [('Â¦mÄ—', 13288), ('ğ‹‡Â¦m', 11491), ('tinÄ—sÂ¦', 10921), ('Â¦taisykl', 10826), ('yn', 10258)]\n",
      "12 1138 8352 [('yr', 12244), ('Â¦kit', 11173), ('Â¦patvirtin', 11031), ('Â¦pabrÄ—Å¾', 10450), ('Â¦social', 10109)]\n",
      "13 1207 7551 [('Â¦inter', 9970), ('ej', 9949), ('Â¦transp', 9182), ('bÅ«', 9109), ('Â¦Å¾e', 8650)]\n",
      "14 1268 6916 [('gu', 9980), ('Â¦rez', 9080), ('Â¦dv', 8474), ('Â¦iniciatyv', 7752), ('ulta', 7713)]\n",
      "15 1333 6367 [('urop', 12021), ('Â¦euro', 10999), ('ğ‹‡Â¦e', 10860), ('Â¦kar', 9253), ('vej', 8009)]\n",
      "16 1405 5727 [('Â¦fr', 9755), ('Â¦rezulta', 7972), ('Ä—ja', 6727), ('Â¦atÅ¾vilgiuÂ¦', 6674), ('Â¦nacional', 6527)]\n",
      "17 1472 5281 [('Â¦poveik', 7899), ('gosÂ¦', 6528), ('Ä—mes', 5988), ('Â¦koleg', 5825), ('ğ‹‡Â¦sÄ…jungojeÂ¦', 5726)]\n",
      "18 1548 4978 [('ganiz', 8704), ('Â¦orga', 8515), ('Â¦svarst', 6433), ('taik', 6169), ('Â¦pastan', 6118)]\n",
      "19 1615 4691 [('rÄ—', 9154), ('Â¦organiz', 8376), ('Ä—Å¾', 7698), ('Â¦kole', 7437), ('gaÂ¦', 5274)]\n",
      "20 1685 4376 [('Â¦susid', 6598), ('Â¦priorite', 5533), ('Â¦stabil', 4934), ('Â¦pasaul', 4872), ('Â¦spe', 4868)]\n",
      "21 1755 4077 [('brÄ—Å¾', 7365), ('prÄ™', 6840), ('Ä™stiÂ¦', 6390), ('naÅ¡', 5880), ('Â¦pastar', 4779)]\n",
      "22 1835 3805 [('Â¦sut', 5198), ('ğ‹‡Â¦ru', 4653), ('lyg', 4209), ('Â¦elektr', 4121), ('Â¦vaid', 4111)]\n",
      "23 1912 3576 [('ekl', 6460), ('Â¦iÅ¡t', 5000), ('Â¦lÄ—', 4776), ('Ä—Å¡', 4299), ('tek', 3877)]\n",
      "24 1972 3366 [('uomen', 5601), ('anc', 4730), ('Â¦supra', 4398), ('Â¦teritor', 4350), ('timiÂ¦', 4312)]\n",
      "25 2035 3187 [('Â¦atk', 5107), ('Â¦trÅ«k', 3758), ('ğ‹‡Â¦prancÅ«', 3552), ('Â¦iÅ¡tekl', 3515), ('Â¦mechaniz', 3482)]\n",
      "26 2108 3007 [('Â¦daugel', 4006), ('iuiÂ¦', 3628), ('Â¦duomen', 3458), ('aup', 3188), ('veiktiÂ¦', 3186)]\n",
      "27 2183 2852 [('Â¦nus', 4602), ('Â¦ilgalaik', 3405), ('ğŠ£Â¦plÂ¦ğŠ¼', 3160), ('Â¦ypat', 3120), ('ingÅ³Â¦', 3081)]\n",
      "28 2247 2709 [('ytosÂ¦', 3471), ('pen', 3354), ('Â¦id', 3060), ('Â¦prim', 2926), ('vykoÂ¦', 2851)]\n",
      "29 2316 2569 [('bl', 3191), ('els', 2840), ('intasÂ¦', 2796), ('ğŠ£Â¦s', 2707), ('Â¦pasir', 2707)]\n",
      "30 2394 2437 [('Â¦pal', 3846), ('Â¦prot', 3577), ('Â¦istor', 3210), ('Â¦fr', 2905), ('Â¦pon', 2864)]\n",
      "31 2463 2323 [('ac', 3004), ('Â¦mlrdÂ¦', 2551), ('Â¦kapit', 2478), ('eitiÂ¦', 2469), ('Â¦kl', 2453)]\n",
      "32 2535 2233 [('Â¦pi', 3885), ('Â¦trans', 3382), ('Â¦dir', 3205), ('Â¦didÅ¾', 2750), ('Â¦skurd', 2585)]\n",
      "33 2607 2145 [('Â¦rizik', 3776), ('lank', 3466), ('Â¦rekomenda', 2995), ('Â¦gyvyb', 2347), ('Â¦pala', 2295)]\n",
      "34 2672 2058 [('ktyv', 3958), ('ğ‹‡Â¦valst', 2678), ('..', 2518), ('Â¦Å¾iniasklaid', 2184), ('Â¦taigiÂ¦', 2173)]\n",
      "35 2744 1973 [('Â¦diskrimin', 2571), ('Â¦civil', 2552), ('bil', 2461), ('iasiÂ¦', 2404), ('Â¦kriter', 2395)]\n",
      "36 2811 1895 [('Â¦situ', 2672), ('..', 2518), ('dÅ¾iaÂ¦', 2126), ('tinÄ—msÂ¦', 2080), ('tern', 2032)]\n",
      "37 2885 1826 [('Â¦sava', 3301), ('ğ‹‡Â¦vokie', 2785), ('Â¦c', 2502), ('Â¦atnauj', 2364), ('Â¦kitiÂ¦', 2112)]\n",
      "38 2959 1757 [('..', 2518), ('umusÂ¦', 1831), ('Â¦nepaka', 1825), ('Â¦sutart', 1824), ('Â¦naf', 1821)]\n",
      "39 3036 1698 [('Â¦iÅ¡t', 3111), ('irÅ«p', 2295), ('Â¦biudÅ¾e', 2149), ('Â¦nepritar', 1970), ('Â¦reg', 1895)]\n",
      "40 3105 1643 [('iamÂ¦', 2652), ('uod', 2520), ('..', 2518), ('Â¦Å¡v', 1984), ('acijÄ…Â¦', 1816)]\n",
      "41 3182 1589 [('Â¦am', 2306), ('Â¦pam', 1959), ('aiv', 1958), ('Â¦med', 1802), ('Â¦apriboj', 1749)]\n",
      "42 3261 1535 [('..', 2518), ('ğ‹‡Â¦rumun', 2476), ('Â¦nel', 1818), ('Â¦int', 1650), ('zd', 1647)]\n",
      "43 3334 1476 [('Â¦prid', 1797), ('Â¦gird', 1542), ('Â¦sl', 1535), ('Â¦rÅ«', 1535), ('Â¦narioÂ¦', 1535)]\n",
      "44 3417 1426 [('Å«p', 2575), ('..', 2518), ('Â¦pasir', 1802), ('aig', 1666), ('ğŠ£Â¦f', 1639)]\n",
      "45 3487 1371 [('skai', 2281), ('ties', 1655), ('ryÅ¾', 1518), ('Â¦iÅ¡gir', 1454), ('Â¦poreikiusÂ¦', 1426)]\n",
      "46 3555 1327 [('Â¦migr', 2241), ('Â¦lÄ—', 1700), ('Â¦imigr', 1478), ('Â¦tero', 1464), ('ğ‹‡Â¦vengrijosÂ¦', 1380)]\n",
      "47 3635 1285 [('oliu', 2521), ('nom', 1908), ('Â¦sveik', 1902), ('Â¦rez', 1810), ('Â¦ekon', 1578)]\n",
      "48 3711 1245 [('eiv', 1897), ('Â¦tÅ«kst', 1624), ('Â¦ekonom', 1434), ('gÄ…Â¦', 1334), ('iamusÂ¦', 1285)]\n",
      "49 3787 1211 [('metÂ¦', 2203), ('acijÅ³Â¦', 1952), ('ğ‹‡Â¦jungt', 1374), ('Â¦priei', 1374), ('Â¦asoci', 1295)]\n",
      "50 3866 1176 [('Â¦visa', 1692), ('sap', 1375), ('Â¦pamat', 1368), ('iÅ¡k', 1332), ('Â¦argumen', 1285)]\n",
      "51 3944 1136 [('Â¦pab', 1892), ('inimÅ³Â¦', 1285), ('Â¦deklar', 1204), ('ğŠ£Â¦huÂ¦ğŠ¼', 1185), ('baig', 1180)]\n",
      "52 4021 1095 [('Â¦pre', 1821), ('rez', 1626), ('iden', 1449), ('kuli', 1385), ('zid', 1269)]\n",
      "Training time: 563.272607088089\n",
      "lt text tokenized in 244.2498173713684 with lt_hft_4.0k\n",
      "Made freqs for lt_hft_4.0k in 0.009147882461547852\n",
      "Training tokenizer for lt with vocab_size of 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 316 193385 [('sÂ¦', 2761024), ('iÂ¦', 1543187), ('in', 1215677), ('ti', 1158230), ('ai', 1078641)]\n",
      "2 409 70537 [('Â¦irÂ¦', 448947), ('tiÂ¦', 313572), ('osÂ¦', 304497), ('Ä—sÂ¦', 250448), ('ğ‹‡Â¦', 232874)]\n",
      "3 494 43472 [('Â¦europ', 147935), ('ğ‹‡Â¦eu', 144447), ('roposÂ¦', 123993), ('Â¦dÄ—lÂ¦', 102298), ('Â¦uÅ¾', 97505)]\n",
      "4 568 30126 [('ğ‹‡Â¦europosÂ¦', 122366), ('iÅ³Â¦', 65133), ('Â¦valstyb', 58091), ('Â¦pirminink', 54661), ('Â¦veik', 46901)]\n",
      "5 634 24265 [('ğ‹‡Â¦sÄ…jung', 43293), ('Â¦pirmininkeÂ¦', 36737), ('Â¦todÄ—lÂ¦', 33717), ('imasÂ¦', 32930), ('Â¦mÅ«sÅ³Â¦', 32847)]\n",
      "6 710 19297 [('Â¦20', 33039), ('nink', 28363), ('Â¦eu', 28090), ('ğ‹‡Â¦e', 27025), ('Â¦sieki', 26076)]\n",
      "7 786 16304 [('aiÂ¦', 25802), ('Â¦dis', 23784), ('Â¦reikal', 23608), ('Â¦sud', 22800), ('ğ‹‡Â¦europ', 22254)]\n",
      "8 858 13804 [('dÄ—lÂ¦', 19555), ('Â¦gali', 18585), ('Â¦ypaÄÂ¦', 18379), ('Â¦politik', 16854), ('Â¦min', 16746)]\n",
      "9 929 12044 [('Â¦tu', 15793), ('Â¦bendradarbi', 14730), ('ğ‹‡Â¦v', 14586), ('Â¦vad', 13984), ('Â¦tarptau', 13939)]\n",
      "10 996 10225 [('Â¦aplink', 14585), ('Â¦sektor', 12918), ('iniÅ³Â¦', 12726), ('Â¦ga', 12320), ('Â¦arbaÂ¦', 12311)]\n",
      "11 1068 9275 [('Â¦mÄ—', 13288), ('ğ‹‡Â¦m', 11491), ('tinÄ—sÂ¦', 10921), ('Â¦taisykl', 10826), ('yn', 10258)]\n",
      "12 1138 8352 [('yr', 12244), ('Â¦kit', 11173), ('Â¦patvirtin', 11031), ('Â¦pabrÄ—Å¾', 10450), ('Â¦social', 10109)]\n",
      "13 1207 7551 [('Â¦inter', 9970), ('ej', 9949), ('Â¦transp', 9182), ('bÅ«', 9109), ('Â¦Å¾e', 8650)]\n",
      "14 1268 6916 [('gu', 9980), ('Â¦rez', 9080), ('Â¦dv', 8474), ('Â¦iniciatyv', 7752), ('ulta', 7713)]\n",
      "15 1333 6367 [('urop', 12021), ('Â¦euro', 10999), ('ğ‹‡Â¦e', 10860), ('Â¦kar', 9253), ('vej', 8009)]\n",
      "16 1405 5727 [('Â¦fr', 9755), ('Â¦rezulta', 7972), ('Ä—ja', 6727), ('Â¦atÅ¾vilgiuÂ¦', 6674), ('Â¦nacional', 6527)]\n",
      "17 1472 5281 [('Â¦poveik', 7899), ('gosÂ¦', 6528), ('Ä—mes', 5988), ('Â¦koleg', 5825), ('ğ‹‡Â¦sÄ…jungojeÂ¦', 5726)]\n",
      "18 1548 4978 [('ganiz', 8704), ('Â¦orga', 8515), ('Â¦svarst', 6433), ('taik', 6169), ('Â¦pastan', 6118)]\n",
      "19 1615 4691 [('rÄ—', 9154), ('Â¦organiz', 8376), ('Ä—Å¾', 7698), ('Â¦kole', 7437), ('gaÂ¦', 5274)]\n",
      "20 1685 4376 [('Â¦susid', 6598), ('Â¦priorite', 5533), ('Â¦stabil', 4934), ('Â¦pasaul', 4872), ('Â¦spe', 4868)]\n",
      "21 1755 4077 [('brÄ—Å¾', 7365), ('prÄ™', 6840), ('Ä™stiÂ¦', 6390), ('naÅ¡', 5880), ('Â¦pastar', 4779)]\n",
      "22 1835 3805 [('Â¦sut', 5198), ('ğ‹‡Â¦ru', 4653), ('lyg', 4209), ('Â¦elektr', 4121), ('Â¦vaid', 4111)]\n",
      "23 1912 3576 [('ekl', 6460), ('Â¦iÅ¡t', 5000), ('Â¦lÄ—', 4776), ('Ä—Å¡', 4299), ('tek', 3877)]\n",
      "24 1972 3366 [('uomen', 5601), ('anc', 4730), ('Â¦supra', 4398), ('Â¦teritor', 4350), ('timiÂ¦', 4312)]\n",
      "25 2035 3187 [('Â¦atk', 5107), ('Â¦trÅ«k', 3758), ('ğ‹‡Â¦prancÅ«', 3552), ('Â¦iÅ¡tekl', 3515), ('Â¦mechaniz', 3482)]\n",
      "26 2108 3007 [('Â¦daugel', 4006), ('iuiÂ¦', 3628), ('Â¦duomen', 3458), ('aup', 3188), ('veiktiÂ¦', 3186)]\n",
      "27 2183 2852 [('Â¦nus', 4602), ('Â¦ilgalaik', 3405), ('ğŠ£Â¦plÂ¦ğŠ¼', 3160), ('Â¦ypat', 3120), ('ingÅ³Â¦', 3081)]\n",
      "28 2247 2709 [('ytosÂ¦', 3471), ('pen', 3354), ('Â¦id', 3060), ('Â¦prim', 2926), ('vykoÂ¦', 2851)]\n",
      "29 2316 2569 [('bl', 3191), ('els', 2840), ('intasÂ¦', 2796), ('ğŠ£Â¦s', 2707), ('Â¦pasir', 2707)]\n",
      "30 2394 2437 [('Â¦pal', 3846), ('Â¦prot', 3577), ('Â¦istor', 3210), ('Â¦fr', 2905), ('Â¦pon', 2864)]\n",
      "31 2463 2323 [('ac', 3004), ('Â¦mlrdÂ¦', 2551), ('Â¦kapit', 2478), ('eitiÂ¦', 2469), ('Â¦kl', 2453)]\n",
      "32 2535 2233 [('Â¦pi', 3885), ('Â¦trans', 3382), ('Â¦dir', 3205), ('Â¦didÅ¾', 2750), ('Â¦skurd', 2585)]\n",
      "33 2607 2145 [('Â¦rizik', 3776), ('lank', 3466), ('Â¦rekomenda', 2995), ('Â¦gyvyb', 2347), ('Â¦pala', 2295)]\n",
      "34 2672 2058 [('ktyv', 3958), ('ğ‹‡Â¦valst', 2678), ('..', 2518), ('Â¦Å¾iniasklaid', 2184), ('Â¦taigiÂ¦', 2173)]\n",
      "35 2744 1973 [('Â¦diskrimin', 2571), ('Â¦civil', 2552), ('bil', 2461), ('iasiÂ¦', 2404), ('Â¦kriter', 2395)]\n",
      "36 2811 1895 [('Â¦situ', 2672), ('..', 2518), ('dÅ¾iaÂ¦', 2126), ('tinÄ—msÂ¦', 2080), ('tern', 2032)]\n",
      "37 2885 1826 [('Â¦sava', 3301), ('ğ‹‡Â¦vokie', 2785), ('Â¦c', 2502), ('Â¦atnauj', 2364), ('Â¦kitiÂ¦', 2112)]\n",
      "38 2959 1757 [('..', 2518), ('umusÂ¦', 1831), ('Â¦nepaka', 1825), ('Â¦sutart', 1824), ('Â¦naf', 1821)]\n",
      "39 3036 1698 [('Â¦iÅ¡t', 3111), ('irÅ«p', 2295), ('Â¦biudÅ¾e', 2149), ('Â¦nepritar', 1970), ('Â¦reg', 1895)]\n",
      "40 3105 1643 [('iamÂ¦', 2652), ('uod', 2520), ('..', 2518), ('Â¦Å¡v', 1984), ('acijÄ…Â¦', 1816)]\n",
      "41 3182 1589 [('Â¦am', 2306), ('Â¦pam', 1959), ('aiv', 1958), ('Â¦med', 1802), ('Â¦apriboj', 1749)]\n",
      "42 3261 1535 [('..', 2518), ('ğ‹‡Â¦rumun', 2476), ('Â¦nel', 1818), ('Â¦int', 1650), ('zd', 1647)]\n",
      "43 3334 1476 [('Â¦prid', 1797), ('Â¦gird', 1542), ('Â¦sl', 1535), ('Â¦rÅ«', 1535), ('Â¦narioÂ¦', 1535)]\n",
      "44 3417 1426 [('Å«p', 2575), ('..', 2518), ('Â¦pasir', 1802), ('aig', 1666), ('ğŠ£Â¦f', 1639)]\n",
      "45 3487 1371 [('skai', 2281), ('ties', 1655), ('ryÅ¾', 1518), ('Â¦iÅ¡gir', 1454), ('Â¦poreikiusÂ¦', 1426)]\n",
      "46 3555 1327 [('Â¦migr', 2241), ('Â¦lÄ—', 1700), ('Â¦imigr', 1478), ('Â¦tero', 1464), ('ğ‹‡Â¦vengrijosÂ¦', 1380)]\n",
      "47 3635 1285 [('oliu', 2521), ('nom', 1908), ('Â¦sveik', 190Training time: 837.0509376525879\n",
      "lt text tokenized in 268.7896318435669 with lt_hft_6.0k\n",
      "Made freqs for lt_hft_6.0k in 0.012288331985473633\n",
      "Training tokenizer for lt with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1163.3695390224457\n",
      "lt text tokenized in 273.638902425766 with lt_hft_8.0k\n",
      "Made freqs for lt_hft_8.0k in 0.017820119857788086\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/3501759698.py:269: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "            #'en_hi',\n",
    "            'lt_en'\n",
    "           ]\n",
    "model_types = [\n",
    "              #'char',\n",
    "              #'unigram',\n",
    "              #'bpe',\n",
    "              'hft'\n",
    "              ]\n",
    "               \n",
    "for dataset in datasets:\n",
    "    for model_type in model_types:\n",
    "        print(dataset, model_type)\n",
    "        model = TokBuilder(dataset, model_type=model_type, data_path='./data_big')\n",
    "        model.run(langs=['lt'])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset, just_tgt=False,):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = f'./data_big/{dataset}'\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        self.just_tgt = just_tgt\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        \n",
    "        langs = self.pair\n",
    "        if self.just_tgt:\n",
    "            langs = [langs[1]]\n",
    "       \n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in langs:\n",
    "            tokenizers = {} # tokenizer : ( freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "               # if \"hft\" not in path: #to remove after hftoks implementation\n",
    "\n",
    "                    tokenizer_name = os.path.basename(path)\n",
    "                    freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                    \n",
    "                    if 'hft' in path:\n",
    "                        train = f'{self.dataset_dir}/train/tokenized/hft_pretokenized.{lang}'\n",
    "                    else:\n",
    "                        train = f'{self.dataset_dir}/train.{lang}'\n",
    "                    \n",
    "                    tokenized = f'{self.dataset_dir}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "\n",
    "                    tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "        \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dataset', 'lang', 'tokenizer', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    \n",
    "                freqs_path = paths[lang][tokenizer][0]\n",
    "                tokenized_path = paths[lang][tokenizer][2]\n",
    "\n",
    "                tokenized_text = open(tokenized_path, 'r')\n",
    "   \n",
    "                freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                freqs = list(sorted(freqs.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "\n",
    "                freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                lines = tokenized_text.readlines()\n",
    "\n",
    "                if 'hft' in tokenizer:\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(' ')\n",
    "                        new_line = [i for i in line if i not in [\"ğ‹‡\",\"â–\",\"ğŠ£\",\"ğŠ¼\"]]\n",
    "                        avg_len += len(new_line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "                    \n",
    "                else:    \n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "\n",
    "                if \"unigram\" in tokenizer:\n",
    "                    tokenizer_type = \"unigram\"\n",
    "                elif \"bpe\" in tokenizer:\n",
    "                    tokenizer_type = \"bpe\"\n",
    "                elif \"char\" in tokenizer:\n",
    "                    tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                elif \"hft\" in tokenizer:\n",
    "                    tokenizer_type = \"hft\"\n",
    "                    \n",
    "\n",
    "                row = {\"dataset\" : self.dataset,\n",
    "                       \"lang\" : lang,                  \n",
    "                       \"tokenizer\" : tokenizer_type,\n",
    "                       \"vocab_size\" : vocab_size,\n",
    "                       \"freq@95%\" : freq_at_95,\n",
    "                       \"avg_len\" : avg_len}\n",
    "                df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        with open(f'./{self.dataset}.csv', 'w+') as out:\n",
    "            df.to_csv(out, sep='\\t')\n",
    "        return(df)\n",
    "    \n",
    "    def plot(self, value):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \n",
    "        plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\n",
    "        do not matter\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.collect_stats()\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        ax = sns.lineplot(data=df,\n",
    "                    x=\"vocab_size\", y=value, hue=\"tokenizer\", style=\"tokenizer\",\n",
    "                    ci=None, markers=True, dashes=False, palette=\"tab10\", linewidth=2.5, sort=True)\n",
    "        \n",
    "        ax.set(title={self.dataset},\n",
    "                    xlabel=\"Vocabulary size\",\n",
    "                    ylabel=value,\n",
    "                    )\n",
    "        if value == 'freq@95%':\n",
    "            ax.invert_yaxis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga', just_tgt=False)\n",
    "p2 = Plotter('en_mr', just_tgt=False)\n",
    "p3 = Plotter('en_hi', just_tgt=False)\n",
    "p4 = Plotter('lt_en', just_tgt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f73cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e90de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_3698/4171614918.py:109: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>lang</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>freq@95%</th>\n",
       "      <th>avg_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>char</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>147.02689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>char</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1</td>\n",
       "      <td>139.215199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.789727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.077283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.663789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>65.701799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.711062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.518889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>750.0</td>\n",
       "      <td>3</td>\n",
       "      <td>53.654665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>49.112621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>750.0</td>\n",
       "      <td>4</td>\n",
       "      <td>56.087558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.587533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.279893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.267027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>34</td>\n",
       "      <td>41.670887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.632458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>15</td>\n",
       "      <td>42.897781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.974623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.469938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.526647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.800137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1319</td>\n",
       "      <td>34.341784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.371897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1702</td>\n",
       "      <td>34.364666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.057991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>771</td>\n",
       "      <td>32.212838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.919863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1123</td>\n",
       "      <td>31.873884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.106094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.907012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.336323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>357</td>\n",
       "      <td>30.149059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>592</td>\n",
       "      <td>28.954198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.224677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>hft</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>372</td>\n",
       "      <td>27.370101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>unigram</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.371962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.90019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>unigram</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.460642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>hft</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>191</td>\n",
       "      <td>29.105662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.216605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.354925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>en</td>\n",
       "      <td>bpe</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.73414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lt_en</td>\n",
       "      <td>lt</td>\n",
       "      <td>bpe</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.368444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset lang tokenizer vocab_size freq@95%     avg_len\n",
       "30   lt_en   en      char      240.0        1   147.02689\n",
       "4    lt_en   lt      char      262.0        1  139.215199\n",
       "38   lt_en   en       hft      500.0        2   64.789727\n",
       "27   lt_en   en   unigram      500.0        1   56.077283\n",
       "24   lt_en   en       bpe      500.0        1   57.663789\n",
       "12   lt_en   lt       hft      500.0        2   65.701799\n",
       "14   lt_en   lt       bpe      500.0        1   61.711062\n",
       "16   lt_en   lt   unigram      500.0        1   61.518889\n",
       "43   lt_en   en       hft      750.0        3   53.654665\n",
       "37   lt_en   en   unigram      750.0        1   49.112621\n",
       "3    lt_en   lt       hft      750.0        4   56.087558\n",
       "17   lt_en   lt   unigram      750.0        1   54.587533\n",
       "25   lt_en   en       bpe     1000.0        1   47.279893\n",
       "13   lt_en   lt       bpe     1000.0        1   50.267027\n",
       "32   lt_en   en       hft     1500.0       34   41.670887\n",
       "22   lt_en   en   unigram     1500.0        1   40.632458\n",
       "0    lt_en   lt       hft     1500.0       15   42.897781\n",
       "5    lt_en   lt   unigram     1500.0        1   44.974623\n",
       "35   lt_en   en       bpe     2000.0        1   39.469938\n",
       "1    lt_en   lt       bpe     2000.0        1   41.526647\n",
       "28   lt_en   en   unigram     3000.0        1   34.800137\n",
       "33   lt_en   en       hft     3000.0     1319   34.341784\n",
       "10   lt_en   lt   unigram     3000.0        1   37.371897\n",
       "15   lt_en   lt       hft     3000.0     1702   34.364666\n",
       "31   lt_en   en   unigram     4000.0        1   33.057991\n",
       "29   lt_en   en       hft     4000.0      771   32.212838\n",
       "7    lt_en   lt   unigram     4000.0        1   34.919863\n",
       "6    lt_en   lt       hft     4000.0     1123   31.873884\n",
       "23   lt_en   en       bpe     4000.0        1   34.106094\n",
       "19   lt_en   lt       bpe     4000.0        1   34.907012\n",
       "8    lt_en   lt   unigram     6000.0        1     32.0352\n",
       "42   lt_en   en   unigram     6000.0        1   31.336323\n",
       "40   lt_en   en       hft     6000.0      357   30.149059\n",
       "2    lt_en   lt       hft     6000.0      592   28.954198\n",
       "9    lt_en   lt       bpe     8000.0        1   30.224677\n",
       "20   lt_en   lt       hft     8000.0      372   27.370101\n",
       "18   lt_en   lt   unigram     8000.0        1   30.371962\n",
       "36   lt_en   en       bpe     8000.0        1    30.90019\n",
       "41   lt_en   en   unigram     8000.0        1   30.460642\n",
       "39   lt_en   en       hft     8000.0      191   29.105662\n",
       "21   lt_en   lt       bpe    16000.0        1   27.216605\n",
       "26   lt_en   en       bpe    16000.0        1   29.354925\n",
       "34   lt_en   en       bpe    32000.0        1    28.73414\n",
       "11   lt_en   lt       bpe    32000.0        1   25.368444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4.collect_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
