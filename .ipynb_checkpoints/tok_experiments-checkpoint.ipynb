{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE v0.1\n",
    "\n",
    "REFERENCE:\n",
    "https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "EN-DE : Euparl, News, CCrawl, \n",
    "DE-EN : idem\n",
    "\n",
    "EN-LT : Eupar\n",
    "\n",
    "EN-MR : LoRes21\n",
    "EN-GA : LoRes21\n",
    "\n",
    "tokenized with sacremoses\n",
    "\n",
    "Lenght mu = arithmetic mean of target seqs after encoding\n",
    "Freq@95% = least freq in the 95% of vocab (log)\n",
    "\n",
    "vocab_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 48000, 64000]\n",
    "\n",
    "!!! BPE and SentPiece voc_size are not comparable, SentPiece gives error over max value that changes with data\n",
    "    > limit at 8k\n",
    "    > bacause of max number of unique forms in train data?\n",
    "    > clarify vocab_size vs merge operations. does sentencepiece uses vocab or merge for bpe implementation?\n",
    "    \n",
    "!!! in original paper tokenized with Sacremoses but it has no option to change voc_size\n",
    "    > sentencepiece has a bpe mode and char, test that and adapt functions and vocab_sizes to accomodate that\n",
    "        > sentencepiece implementation is good enough\n",
    "        \n",
    "!!! get logs of training and tokenization speed and other output to df and save csv for final run\n",
    "    > check the correctness of freq@95 and avg_len stats\n",
    "\n",
    "!!! implement hft in class\n",
    "    > kind of works, problems with ga pretokenized\n",
    "    > finish implementation, convert outputs to dict for implementation\n",
    "    \n",
    "!!! better plots\n",
    "    > find good variables to correlate\n",
    "\n",
    "\n",
    "build BleuTester with trained NMT\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import ast\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokBuilder:\n",
    "    \"\"\"\n",
    "    builds tokenizers and freq dict for lang pair. can use unigram, bpe, char model_type\n",
    "    \"\"\"\n",
    "    def __init__(self, pair, model_type):\n",
    "        self.pair = pair\n",
    "        self.langs = pair.split(\"_\")\n",
    "        self.src_lang = self.langs[0]\n",
    "        self.tgt_lang = self.langs[1]\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def count_chars(self, lang):\n",
    "        \"\"\"\n",
    "        returns number of unique chars in file for char vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "\n",
    "        with open(file_path, 'r') as file:   \n",
    "            unique = []\n",
    "\n",
    "            for line in file.readlines():\n",
    "                for char in line:\n",
    "                    if char not in unique:\n",
    "                        unique.append(char)\n",
    "\n",
    "        return int(len(unique))\n",
    "\n",
    "    \n",
    "    def make_batches(self, lang):\n",
    "        \"\"\"\n",
    "        Makes batches of 5_000 lines from bigger txt file for the selectet lang\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}'\n",
    "        file = open(file_path, 'r')\n",
    "        data = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            \n",
    "            save_path = f'./data/{self.src_lang}_{self.tgt_lang}/train/text/train_{file_count}.{lang}'\n",
    "            \n",
    "            if len(text_data) == 5_000:\n",
    "                # once we hit the 5K mark, save to file\n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "\n",
    "        with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "    \n",
    "    def gather_files(self, lang):\n",
    "        \"\"\"\n",
    "        Returns the paths to the training batches for the selected lang\n",
    "        \"\"\"\n",
    "        \n",
    "        self.make_batches(lang)\n",
    "        paths = [str(x) for x in Path(f'./data/{self.src_lang}_{self.tgt_lang}/train/text').glob(f'**/*.{lang}')]\n",
    "        return paths\n",
    "\n",
    "    def train_tokenizer(self, lang, vocab_size):\n",
    "        \"\"\"\n",
    "        Trains a SentencePiece tokenizer for the selected lang and vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'Training tokenizer for {lang} with vocab_size of {vocab_size}')\n",
    "        \n",
    "        tokenizer_name = f'{lang}_{self.model_type}_{vocab_size/1000}k'\n",
    "        \n",
    "        sp_model = spm.SentencePieceProcessor()\n",
    "              \n",
    "        paths = self.gather_files(lang)\n",
    "        \n",
    "        #the revoming part gets the script stuck after a while. removing manually makes it work\n",
    "        \n",
    "        tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer_name}'\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}')\n",
    "        \n",
    "        if not os.path.isdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}'):\n",
    "            os.mkdir(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}')\n",
    "        \n",
    "        if os.path.isdir(tokenizer_path):\n",
    "            shutil.rmtree(tokenizer_path)        \n",
    "        \n",
    "        os.mkdir(tokenizer_path)\n",
    "        \n",
    "        if self.model_type == 'hft':\n",
    "            \n",
    "            cmd0 = f'./pretokenize ./data/{self.pair}/train.{lang} > ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang}' \n",
    "            cmd1 = f'./hftoks.py learn ./data/{self.pair}/train/tokenized/hft_pretokenized.{lang} {tokenizer_path}/{tokenizer_name}.vocab {vocab_size} 100'\n",
    "            start = time.time()\n",
    "            os.system(cmd0)\n",
    "            os.system(cmd1)\n",
    "            end = time.time()\n",
    "        \n",
    "        else:\n",
    "            start = time.time()\n",
    "\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                input=paths,\n",
    "                model_prefix=f'{tokenizer_path}/{tokenizer_name}',\n",
    "                vocab_size=vocab_size,\n",
    "                unk_id=2,\n",
    "                bos_id=-1,\n",
    "                eos_id=1,\n",
    "                pad_id=0,\n",
    "                model_type=self.model_type,\n",
    "                train_extremely_large_corpus=False\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "        \n",
    "        print(f'Training time: {end-start}')\n",
    "\n",
    "        print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "       \n",
    "    def make_freqs(self, lang, tokenizer, save_tokenized=False):\n",
    "        \"\"\"\n",
    "        Makes frequency files for the selected lang and tokenizer\n",
    "        \"\"\"\n",
    "        if self.model_type == 'hft':    \n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.vocab'\n",
    "            \n",
    "            \n",
    "            \n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            tokenizer_path = f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.model'\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.load(f'{tokenizer_path}')\n",
    "\n",
    "            vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "\n",
    "            if save_tokenized == True:\n",
    "                if os.path.isfile(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}'):\n",
    "                    os.remove(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}')\n",
    "\n",
    "            freq = {}\n",
    "            with open(f'./data/{self.src_lang}_{self.tgt_lang}/train.{lang}', 'r') as f:\n",
    "                start = time.time()\n",
    "                for line in f:\n",
    "                    line = line.rstrip()\n",
    "                    toks = sp.encode_as_pieces(line)\n",
    "                    for piece in toks:\n",
    "                        freq.setdefault(piece, 0)\n",
    "                        freq[piece] += 1\n",
    "\n",
    "                    if save_tokenized == True:\n",
    "                        with open(f'./data/{self.src_lang}_{self.tgt_lang}/train/tokenized/toks_{tokenizer}.{lang}', 'a+') as out:\n",
    "                            print(toks, file=out)\n",
    "                end = time.time()\n",
    "\n",
    "            freq = sorted(freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "            with open(f'./tokenizers/{self.src_lang}_{self.tgt_lang}/{lang}/{tokenizer}/{tokenizer}.freq', 'w+') as out:\n",
    "                print(freq, file=out)\n",
    "\n",
    "            print(f'{lang} text tokenized in {end-start} with {tokenizer}')\n",
    "            print(\"\\n }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \\n\")\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the training and frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        langs = [self.src_lang, self.tgt_lang]\n",
    "       \n",
    "        for lang in langs:\n",
    "            \n",
    "            if self.model_type=='char':\n",
    "                vocab_sizes = [self.count_chars(lang)]\n",
    "            elif self.model_type=='bpe': #merge operations\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000, \n",
    "                               16000,\n",
    "                               32000, \n",
    "                               #48000, too big for en-ga\n",
    "                               #64000 too big for en-mr\n",
    "                               ]\n",
    "            elif self.model_type=='unigram': #final vocabulary size\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000\n",
    "                               ]\n",
    "            elif self.model_type=='hft': #final vocabulary size\n",
    "                vocab_sizes = [500,\n",
    "                               1000,\n",
    "                               2000,\n",
    "                               4000,\n",
    "                               8000\n",
    "                               ]\n",
    "            \n",
    "            for size in vocab_sizes:\n",
    "                \n",
    "                    self.train_tokenizer(lang, size)\n",
    "                    tokenizer_name = f'{lang}_{self.model_type}_{size/1000}k'\n",
    "                    self.make_freqs(lang, tokenizer_name, save_tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d439bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ga_char = TokBuilder('en_ga',  model_type='hft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45f3c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer for en with vocab_size of 500\n",
      "Training time: 2.335566759109497\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 1000\n",
      "Training time: 4.75471305847168\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 2000\n",
      "Training time: 12.298535346984863\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 4000\n",
      "Training time: 27.994532823562622\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for en with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 303, in <module>\n",
      "    vocab = process_text(words, vocab_size=vocab_size, step_size=step_size)\n",
      "  File \"./hftoks.py\", line 240, in process_text\n",
      "    nextvocab, candid = vocab_from_segments(allwords, vocab)\n",
      "  File \"./hftoks.py\", line 156, in vocab_from_segments\n",
      "    segs = best_toks(w, vocab)\n",
      "  File \"./hftoks.py\", line 212, in best_toks\n",
      "    if this.is_better(scores[i+1]):\n",
      "  File \"./hftoks.py\", line 180, in is_better\n",
      "    if self.cnt < that.cnt:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 61.009546756744385\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for ga with vocab_size of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 302, in <module>\n",
      "    words = open(sys.argv[2], encoding='utf8').read().split()\n",
      "  File \"/usr/lib/python3.8/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 622591: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.2871432304382324\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for ga with vocab_size of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 302, in <module>\n",
      "    words = open(sys.argv[2], encoding='utf8').read().split()\n",
      "  File \"/usr/lib/python3.8/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 622591: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.28722572326660156\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for ga with vocab_size of 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 303, in <module>\n",
      "    vocab = process_text(words, vocab_size=vocab_size, step_size=step_size)\n",
      "  File \"./hftoks.py\", line 226, in process_text\n",
      "    allwords = word_counts(word_iter)\n",
      "  File \"./hftoks.py\", line 131, in word_counts\n",
      "    words.increment(w, 1)\n",
      "  File \"./hftoks.py\", line 92, in increment\n",
      "    n,s = self.find_prefix(sequence)\n",
      "  File \"./hftoks.py\", line 51, in find_prefix\n",
      "    nxt = this.sub.get(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.3624241352081299\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for ga with vocab_size of 4000\n",
      "Training time: 0.17588591575622559\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n",
      "Training tokenizer for ga with vocab_size of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 303, in <module>\n",
      "    vocab = process_text(words, vocab_size=vocab_size, step_size=step_size)\n",
      "  File \"./hftoks.py\", line 240, in process_text\n",
      "    nextvocab, candid = vocab_from_segments(allwords, vocab)\n",
      "  File \"./hftoks.py\", line 161, in vocab_from_segments\n",
      "    candid.increment(s1 + s2, f)\n",
      "  File \"./hftoks.py\", line 92, in increment\n",
      "    n,s = self.find_prefix(sequence)\n",
      "  File \"./hftoks.py\", line 51, in find_prefix\n",
      "    nxt = this.sub.get(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.31093263626098633\n",
      "\n",
      " }-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{}-<>-{ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./hftoks.py\", line 303, in <module>\n",
      "    vocab = process_text(words, vocab_size=vocab_size, step_size=step_size)\n",
      "  File \"./hftoks.py\", line 229, in process_text\n",
      "    vocab, candidates = vocab_from_bytes(allwords)\n",
      "  File \"./hftoks.py\", line 144, in vocab_from_bytes\n",
      "    for w,f,_ in allwords.items():\n",
      "  File \"./hftoks.py\", line 107, in items\n",
      "    if node.is_final():\n",
      "  File \"./hftoks.py\", line 20, in is_final\n",
      "    return self.ID is not None\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "en_ga_char.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.pair = self.dataset.split('_')\n",
    "        self.dataset_dir = f'./data/{dataset}'\n",
    "        self.tokenizers_dir = f'./tokenizers/{dataset}'\n",
    "        \n",
    "    def collect_paths(self):\n",
    "        paths = {} #lang : {}\n",
    "          \n",
    "        for lang in self.pair:\n",
    "            tokenizers = {} # tokenizer : ( freqs, train, tokenized)\n",
    "            tokenizers_paths = [path for path in os.listdir(f'{self.tokenizers_dir}/{lang}')]\n",
    "            \n",
    "            for path in tokenizers_paths:\n",
    "                \n",
    "                tokenizer_name = os.path.basename(path)\n",
    "                freqs = f'{self.tokenizers_dir}/{lang}/{tokenizer_name}/{tokenizer_name}.freq'\n",
    "                train = f'{self.dataset_dir}/train.{lang}'\n",
    "                tokenized = f'{self.dataset_dir}/train/tokenized/toks_{tokenizer_name}.{lang}'\n",
    "               \n",
    "                tokenizers[path] = (freqs, train, tokenized)\n",
    "            \n",
    "            paths[lang] = tokenizers\n",
    "        \n",
    "        return (paths)\n",
    "        \n",
    "    def collect_stats(self):\n",
    "        \"\"\"\n",
    "        do for all data\n",
    "        \n",
    "        for pair in pairs:\n",
    "            for lang in pair:\n",
    "                for tokenizer in lang_tokenizers:\n",
    "                    collect stats\n",
    "        \n",
    "        return(df)\n",
    "        \"\"\"\n",
    "        \n",
    "        paths = self.collect_paths()\n",
    "        \n",
    "        df = pd.DataFrame(columns=['tokenizer_type', 'vocab_size', 'freq@95%', 'avg_len'])\n",
    "        \n",
    "        last_index = len(df)\n",
    "        for lang in paths.keys():\n",
    "            for tokenizer in paths[lang].keys():\n",
    "                    freqs_path = paths[lang][tokenizer][0]\n",
    "                    tokenized_path = paths[lang][tokenizer][2]\n",
    "                    \n",
    "                    tokenized_text = open(tokenized_path, 'r')\n",
    "                    freqs = ast.literal_eval(open(freqs_path).read())\n",
    "                    freqs = list(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "                    freq_at_95 = freqs[int((len(freqs)/100)*95)][1]\n",
    "\n",
    "                    lines = tokenized_text.readlines()\n",
    "\n",
    "                    avg_len = 0\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.split(',')\n",
    "                        avg_len += len(line)\n",
    "\n",
    "                    avg_len = avg_len/len(lines)\n",
    "\n",
    "                    vocab_size = float(re.sub(r'[^\\d.]+',\"\", tokenizer))*1000\n",
    "                    \n",
    "                    if \"unigram\" in tokenizer:\n",
    "                        tokenizer_type = \"unigram\"\n",
    "                    elif \"bpe\" in tokenizer:\n",
    "                        tokenizer_type = \"bpe\"\n",
    "                    elif \"char\" in tokenizer:\n",
    "                        tokenizer_type = \"char\" #char has just 1 value, add to another type?\n",
    "                    \n",
    "\n",
    "                    df.at[last_index, \"tokenizer_type\"] = tokenizer_type\n",
    "                    df.at[last_index, \"vocab_size\"] = vocab_size\n",
    "                    df.at[last_index, \"freq@95%\"] = freq_at_95\n",
    "                    df.at[last_index, \"avg_len\"] = avg_len\n",
    "                    last_index+=1\n",
    "        \n",
    "        df = df.sort_values(by=\"vocab_size\", axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "        \n",
    "        return(df)\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        returns plot\n",
    "        \n",
    "        plot must be at the end of all data, and have vocab_size on x and freq@95% on y, tokenizer names\n",
    "        do not matter\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.collect_stats()\n",
    "       # df = df.drop('tokenizer', axis=1)\n",
    "        #df = pd.melt(df, ['vocab_size'])\n",
    "        \n",
    "        ax = sns.relplot(data=df,\n",
    "                    x=\"vocab_size\", y=\"freq@95%\", hue=\"tokenizer_type\",\n",
    "                    kind=\"line\", ci=None)\n",
    "        ax.set(title={self.dataset},\n",
    "                    xlabel=\"Vocabulary size\",\n",
    "                    ylabel=\"Freq@95%\",\n",
    "                    )\n",
    "        #ax.set(yscale='log')\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plotter('en_ga')\n",
    "p2 = Plotter('en_mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e80b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot()\n",
    "p2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"generate env var and run from server screen ctrl+a d, and to reconnect screen -r\n",
    "\n",
    "or redirect all the outputs on a file and run the process with nohup and & (running in bg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuTester:\n",
    "    \"\"\"\n",
    "    trains nmt from tokenized with tokenizers,\n",
    "    translates,\n",
    "    computes bleu scores and plots results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pair, tokenizers):\n",
    "        self.pair = pair.split('-')\n",
    "        self.src_lang = self.pair[0]\n",
    "        self.tgt_lang = self.pair[1]\n",
    "        \n",
    "    def tokenize(self, ):\n",
    "        \"\"\"\n",
    "        loads tokenizer, \n",
    "        tokenizes train.lang,\n",
    "        returns tokenized, speed\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_env_var(self, ):\n",
    "        \"\"\"\n",
    "        generate env_vars for current run\n",
    "        \"\"\"\n",
    "        \n",
    "        env_vars = 'export DATA_PATH= ../data\n",
    "\n",
    "        export VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\n",
    "        export VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\n",
    "        export TRAIN_SOURCES=${DATA_PATH}/toks_0.5k.en\n",
    "        export TRAIN_TARGETS=${DATA_PATH}/toks_0.5k.mr\n",
    "        export DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\n",
    "        export DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n",
    "\n",
    "        export DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\n",
    "        export TRAIN_STEPS=1000000'\n",
    "    \n",
    "    def train_nmt(self,):\n",
    "        \"\"\"\n",
    "        loads tokenized,\n",
    "        trains model\n",
    "        \"\"\"\n",
    "        \n",
    "    def translate(self, ):\n",
    "        \"\"\"\n",
    "        loads model,\n",
    "        loads dev or test,\n",
    "        translates\n",
    "        returns translation\n",
    "        \"\"\"\n",
    "    \n",
    "    def compute_bleu(self, ):\n",
    "        \"\"\"\n",
    "        loads translation,\n",
    "        computes bleu,\n",
    "        returns list of bleu scores\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot(self, ):\n",
    "        \"\"\"\n",
    "        plots results\n",
    "        \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        runs the whole thing\n",
    "        \"\"\"class BleuTester:\n",
    "    def __init__(self,):\n",
    "        \n",
    "    def train_nmt(self,)\n",
    "    \n",
    "    def compute_bleu(self,)\n",
    "    \n",
    "    def run(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
