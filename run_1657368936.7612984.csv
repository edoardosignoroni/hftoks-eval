	dataset	lang	tokenizer	vocab_size	train	token
0	lt_en	lt	lt_hft_0.5k	500	65.73120403289795	198.1664822101593
1	lt_en	lt	lt_hft_0.75k	750	88.39507412910461	201.43742966651917
2	lt_en	lt	lt_hft_1.5k	1500	176.7579209804535	213.65005660057068
3	lt_en	lt	lt_hft_3.0k	3000	917.7200179100037	248.20147275924683
4	lt_en	lt	lt_hft_4.0k	4000	563.272607088089	244.2498173713684
5	lt_en	lt	lt_hft_6.0k	6000	837.0509376525879	268.7896318435669
6	lt_en	lt	lt_hft_8.0k	8000	1163.3695390224457	273.638902425766
