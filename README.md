#CURRENT OBJECTIVE

train tokenizers for same sizes as Gowda and May, 2021

>	see notebook fo TODO

reproduce their results and plot them

train HFToks and plot Freq@95% and Length (in toks) vs vocab_size

our Freq@95% should be HIGHER, and Length should be LOWER

>	good for Low Res because of efficiency, saved memory, better use of limited data

If possible then train NMT for each tokenizer and plot that

Copy of LoResMT 2021 shared task to experiment with tokenizer vocabulary size and BLEU scores in Low Res

